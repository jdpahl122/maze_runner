{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c31d15a",
   "metadata": {},
   "source": [
    "\n",
    "# ðŸ§­ Maze Runner RL (PPO + PyTorch + Stable-Baselines3)\n",
    "\n",
    "This notebook trains an agent with **PPO** to navigate **procedurally generated mazes** using **partial observations**.\n",
    "It is GPU-ready and logs to TensorBoard. Works on Linux/macOS/Windows (WSL recommended on Windows).\n",
    "\n",
    "**What you'll get**\n",
    "- A custom `gymnasium` environment (`MazeEnv`) with egocentric 7Ã—7 view\n",
    "- Procedural maze generator (DFS backtracker)\n",
    "- A* shortest-path baseline for evaluation\n",
    "- PPO training (Stable-Baselines3) with CNN policy\n",
    "- Metrics: success rate, average steps, steps vs A* ratio\n",
    "- Optional LSTM policy (partial observability)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041d6452",
   "metadata": {},
   "source": [
    "\n",
    "## 0. Environment setup (run once in your local machine)\n",
    "> **Tip:** Install the correct CUDA-enabled torch first. Pick one of the commands below that matches your system.\n",
    "\n",
    "**Conda (recommended):**\n",
    "```bash\n",
    "# Create env\n",
    "conda create -n maze-rl python=3.11 -y\n",
    "conda activate maze-rl\n",
    "\n",
    "# Install PyTorch with CUDA (choose the right cudatoolkit)\n",
    "# See https://pytorch.org/get-started/locally/ for exact command for your system\n",
    "# Example for CUDA 12.x:\n",
    "pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "# Core libs\n",
    "pip install gymnasium stable-baselines3[extra] tensorboard moviepy imageio imageio[ffmpeg] pygame\n",
    "pip install matplotlib numpy tqdm mlflow  # optional extras\n",
    "```\n",
    "\n",
    "**PyTorch CPU-only (if no NVIDIA GPU):**\n",
    "```bash\n",
    "pip install torch torchvision --index-url https://download.pytorch.org/whl/cpu\n",
    "```\n",
    "Then, start Jupyter:\n",
    "```bash\n",
    "jupyter lab  # or jupyter notebook\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "331a4bdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.13.2 | OS: Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.31\n",
      "PyTorch: 2.8.0+cu128\n",
      "CUDA available: True\n",
      "GPU name: NVIDIA GeForce RTX 2070\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ðŸ”Ž Quick GPU sanity check\n",
    "import torch, platform, sys\n",
    "print(\"Python:\", sys.version.split()[0], \"| OS:\", platform.platform())\n",
    "print(\"PyTorch:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU name:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"NOTE: CUDA not available. Training will run on CPU (much slower).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f3be9305",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Core imports\n",
    "import numpy as np\n",
    "import math, random\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple, List, Optional\n",
    "\n",
    "# Gymnasium\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "# RL\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv, DummyVecEnv\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.callbacks import BaseCallback, EvalCallback\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "# Utils\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio\n",
    "from tqdm import trange\n",
    "import os, time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d689e262",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Maze generation (DFS backtracker)\n",
    "Generates a *perfect* maze (single path between any two points) on odd-sized grids.\n",
    "`1 = wall`, `0 = free space`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "671d7c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_maze(width: int, height: int, rng: np.random.Generator) -> np.ndarray:\n",
    "    \"\"\"Generate a perfect maze with DFS backtracking.\n",
    "    Grid is odd x odd, walls=1, free=0.\"\"\"\n",
    "    w = width if width % 2 == 1 else width + 1\n",
    "    h = height if height % 2 == 1 else height + 1\n",
    "    grid = np.ones((h, w), dtype=np.int8)\n",
    "\n",
    "    def neighbors(r, c):\n",
    "        for dr, dc in [(-2,0),(2,0),(0,-2),(0,2)]:\n",
    "            nr, nc = r+dr, c+dc\n",
    "            if 1 <= nr < h-1 and 1 <= nc < w-1:\n",
    "                yield nr, nc, r+dr//2, c+dc//2  # cell and the wall between\n",
    "\n",
    "    # start at random odd cell\n",
    "    start_r = rng.integers(1, h-1)\n",
    "    start_c = rng.integers(1, w-1)\n",
    "    start_r -= (start_r % 2 == 0)\n",
    "    start_c -= (start_c % 2 == 0)\n",
    "    stack = [(start_r, start_c)]\n",
    "    grid[start_r, start_c] = 0\n",
    "\n",
    "    while stack:\n",
    "        r, c = stack[-1]\n",
    "        nbrs = [(nr, nc, wr, wc) for nr, nc, wr, wc in neighbors(r, c) if grid[nr, nc] == 1]\n",
    "        if nbrs:\n",
    "            nr, nc, wr, wc = nbrs[rng.integers(len(nbrs))]\n",
    "            grid[wr, wc] = 0\n",
    "            grid[nr, nc] = 0\n",
    "            stack.append((nr, nc))\n",
    "        else:\n",
    "            stack.pop()\n",
    "\n",
    "    return grid\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef098fa6",
   "metadata": {},
   "source": [
    "\n",
    "## 2. A* shortest-path baseline\n",
    "Used for evaluation (not available to the agent).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0de51c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import heapq\n",
    "\n",
    "def astar_length(grid: np.ndarray, start: Tuple[int,int], goal: Tuple[int,int]) -> Optional[int]:\n",
    "    H, W = grid.shape\n",
    "    def h(p):  # Manhattan heuristic\n",
    "        return abs(p[0]-goal[0]) + abs(p[1]-goal[1])\n",
    "    open_set = [(h(start), 0, start)]\n",
    "    g = {start: 0}\n",
    "    visited = set()\n",
    "\n",
    "    while open_set:\n",
    "        _, cost, node = heapq.heappop(open_set)\n",
    "        if node in visited:\n",
    "            continue\n",
    "        visited.add(node)\n",
    "        if node == goal:\n",
    "            return cost\n",
    "        r, c = node\n",
    "        for dr, dc in [(-1,0),(1,0),(0,-1),(0,1)]:\n",
    "            nr, nc = r+dr, c+dc\n",
    "            if 0 <= nr < H and 0 <= nc < W and grid[nr, nc] == 0:\n",
    "                new_cost = cost + 1\n",
    "                if new_cost < g.get((nr, nc), 1e9):\n",
    "                    g[(nr, nc)] = new_cost\n",
    "                    heapq.heappush(open_set, (new_cost + h((nr, nc)), new_cost, (nr, nc)))\n",
    "    return None  # unreachable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91aea83b",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Gymnasium Environment (`MazeEnv`)\n",
    "- **Observation:** egocentric 7Ã—7 crop with 4 channels (empty, wall, goal, agent)\n",
    "- **Actions:** 4-way movement\n",
    "- **Rewards:** +1 for goal, -0.01 per step, -0.05 on wall bump, optional shaping toward goal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "075154e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MazeEnv(gym.Env):\n",
    "    metadata = {\"render_modes\": [\"rgb_array\"]}\n",
    "\n",
    "    def __init__(self, size=15, view=7, max_steps=200, seed=None, shaped=True):\n",
    "        super().__init__()\n",
    "        self.size = size\n",
    "        self.view = view\n",
    "        self.max_steps = max_steps\n",
    "        self.shaped = shaped\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "\n",
    "        self.action_space = spaces.Discrete(4)  # U,D,L,R\n",
    "        # 4 channels: empty, wall, goal, agent\n",
    "        self.observation_space = spaces.Box(low=0.0, high=1.0, shape=(4, view, view), dtype=np.float32)\n",
    "\n",
    "        self.grid = None\n",
    "        self.agent = None\n",
    "        self.goal = None\n",
    "        self.steps = 0\n",
    "\n",
    "    def _reset_level(self):\n",
    "        self.grid = generate_maze(self.size, self.size, self.rng)\n",
    "        # pick start/goal on free cells, far apart\n",
    "        free = np.argwhere(self.grid == 0)\n",
    "        a_idx = self.rng.integers(len(free))\n",
    "        g_idx = self.rng.integers(len(free))\n",
    "        self.agent = tuple(free[a_idx])\n",
    "        self.goal = tuple(free[g_idx])\n",
    "        # ensure reachable and reasonably far\n",
    "        for _ in range(200):\n",
    "            L = astar_length(self.grid, self.agent, self.goal)\n",
    "            if L is not None and L > self.size//2:\n",
    "                break\n",
    "            a_idx = self.rng.integers(len(free))\n",
    "            g_idx = self.rng.integers(len(free))\n",
    "            self.agent = tuple(free[a_idx])\n",
    "            self.goal = tuple(free[g_idx])\n",
    "\n",
    "    def _obs(self):\n",
    "        # egocentric crop centered at agent, padded with walls\n",
    "        pad = self.view//2\n",
    "        H, W = self.grid.shape\n",
    "        padded = np.ones((H+2*pad, W+2*pad), dtype=np.int8)\n",
    "        padded[pad:pad+H, pad:pad+W] = self.grid\n",
    "\n",
    "        ar, ac = self.agent\n",
    "        gr, gc = self.goal\n",
    "        crop = padded[ar:ar+2*pad+1, ac:ac+2*pad+1]\n",
    "\n",
    "        empty = (crop == 0).astype(np.float32)\n",
    "        wall  = (crop == 1).astype(np.float32)\n",
    "\n",
    "        goal_layer = np.zeros_like(empty, dtype=np.float32)\n",
    "        ga, gb = pad+(gr-ar), pad+(gc-ac)\n",
    "        if 0 <= ga < self.view and 0 <= gb < self.view:\n",
    "            goal_layer[ga, gb] = 1.0\n",
    "\n",
    "        agent_layer = np.zeros_like(empty, dtype=np.float32)\n",
    "        agent_layer[pad, pad] = 1.0\n",
    "\n",
    "        obs = np.stack([empty, wall, goal_layer, agent_layer], axis=0)\n",
    "        return obs\n",
    "\n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        if seed is not None:\n",
    "            self.rng = np.random.default_rng(seed)\n",
    "        self._reset_level()\n",
    "        self.steps = 0\n",
    "        obs = self._obs()\n",
    "        info = {}\n",
    "        return obs, info\n",
    "\n",
    "    def step(self, action):\n",
    "        self.steps += 1\n",
    "        r, c = self.agent\n",
    "        drdc = [(-1,0),(1,0),(0,-1),(0,1)][int(action)]\n",
    "        nr, nc = r+drdc[0], c+drdc[1]\n",
    "\n",
    "        reward = -0.01  # step penalty\n",
    "        done = False\n",
    "\n",
    "        if 0 <= nr < self.grid.shape[0] and 0 <= nc < self.grid.shape[1] and self.grid[nr, nc] == 0:\n",
    "            # valid move\n",
    "            old_dist = abs(r-self.goal[0]) + abs(c-self.goal[1])\n",
    "            new_dist = abs(nr-self.goal[0]) + abs(nc-self.goal[1])\n",
    "            \n",
    "            # Stronger reward shaping to encourage exploration toward goal\n",
    "            if self.shaped:\n",
    "                if new_dist < old_dist:\n",
    "                    reward += 0.01  # Increased reward for getting closer\n",
    "                elif new_dist > old_dist:\n",
    "                    reward -= 0.005  # Small penalty for moving away\n",
    "            \n",
    "            self.agent = (nr, nc)\n",
    "        else:\n",
    "            # wall bump - stronger penalty to discourage this behavior\n",
    "            reward -= 0.1\n",
    "\n",
    "        if self.agent == self.goal:\n",
    "            reward += 10.0  # Much larger reward for reaching goal\n",
    "            done = True\n",
    "\n",
    "        truncated = self.steps >= self.max_steps\n",
    "        obs = self._obs()\n",
    "        info = {}\n",
    "        return obs, reward, done, truncated, info\n",
    "\n",
    "    def render(self):\n",
    "        # simple top-down RGB\n",
    "        H, W = self.grid.shape\n",
    "        img = np.zeros((H, W, 3), dtype=np.uint8)\n",
    "        img[self.grid == 1] = (30, 30, 30)    # walls\n",
    "        img[self.grid == 0] = (220, 220, 220) # floor\n",
    "        gr, gc = self.goal\n",
    "        img[gr, gc] = (50, 190, 60)           # goal (green)\n",
    "        ar, ac = self.agent\n",
    "        img[ar, ac] = (30, 144, 255)          # agent (blue)\n",
    "        return img\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9d75d1",
   "metadata": {},
   "source": [
    "\n",
    "### Quick visual sanity check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1a247335",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "env = MazeEnv(size=15, view=7, max_steps=200, seed=42)\n",
    "obs, info = env.reset()\n",
    "plt.imshow(env.render())\n",
    "plt.title(\"Sample Maze (blue=agent, green=goal, dark=walls)\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88bcf6d0",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Vectorized environments for faster training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "94390e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_env(seed_offset=0, size=25):  # Increased from 15 to 25\n",
    "    def thunk():\n",
    "        # Scale max_steps with maze size for larger mazes\n",
    "        max_steps = size * size // 2  # More steps for larger mazes\n",
    "        env = MazeEnv(size=size, view=7, max_steps=max_steps, seed=int(time.time())+seed_offset)\n",
    "        return Monitor(env)\n",
    "    return thunk\n",
    "\n",
    "# Progressive maze sizes - start smaller and work up\n",
    "MAZE_SIZE = 21  # Start with 21, then scale to 25, 31, 41\n",
    "NUM_ENVS = 8   # More environments for complex mazes\n",
    "vec_env = DummyVecEnv([make_env(i, size=MAZE_SIZE) for i in range(NUM_ENVS)])\n",
    "test_env = DummyVecEnv([make_env(10_000, size=MAZE_SIZE)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1d91f5",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Train PPO (CNN policy)\n",
    "Logs go to `./tb_logs/ppo_maze` â€“ you can launch TensorBoard to watch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a7528890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 20,000 | Episodes: 100 | Success Rate: 0.00 | Mean Reward: -6.65 | Mean Length: 197.7\n",
      "Step 20,000 | Episodes: 100 | Success Rate: 0.00 | Mean Reward: -6.65 | Mean Length: 197.7\n",
      "Step 40,000 | Episodes: 100 | Success Rate: 0.10 | Mean Reward: -4.80 | Mean Length: 185.8\n",
      "Step 20,000 | Episodes: 100 | Success Rate: 0.00 | Mean Reward: -6.65 | Mean Length: 197.7\n",
      "Step 40,000 | Episodes: 100 | Success Rate: 0.10 | Mean Reward: -4.80 | Mean Length: 185.8\n",
      "Step 60,000 | Episodes: 100 | Success Rate: 0.05 | Mean Reward: -3.93 | Mean Length: 194.3\n",
      "Step 20,000 | Episodes: 100 | Success Rate: 0.00 | Mean Reward: -6.65 | Mean Length: 197.7\n",
      "Step 40,000 | Episodes: 100 | Success Rate: 0.10 | Mean Reward: -4.80 | Mean Length: 185.8\n",
      "Step 60,000 | Episodes: 100 | Success Rate: 0.05 | Mean Reward: -3.93 | Mean Length: 194.3\n",
      "Step 80,000 | Episodes: 100 | Success Rate: 0.10 | Mean Reward: -1.43 | Mean Length: 187.2\n",
      "Step 20,000 | Episodes: 100 | Success Rate: 0.00 | Mean Reward: -6.65 | Mean Length: 197.7\n",
      "Step 40,000 | Episodes: 100 | Success Rate: 0.10 | Mean Reward: -4.80 | Mean Length: 185.8\n",
      "Step 60,000 | Episodes: 100 | Success Rate: 0.05 | Mean Reward: -3.93 | Mean Length: 194.3\n",
      "Step 80,000 | Episodes: 100 | Success Rate: 0.10 | Mean Reward: -1.43 | Mean Length: 187.2\n",
      "Step 100,000 | Episodes: 100 | Success Rate: 0.00 | Mean Reward: -2.18 | Mean Length: 200.0\n",
      "\u001b[F\u001b[K\u001b[F\u001b[K\u001b[F\u001b[K\u001b[F\u001b[K\u001b[F\u001b[KStep 40,000 | Episodes: 100 | Success Rate: 0.10 | Mean Reward: -4.80 | Mean Length: 185.8\n",
      "Step 60,000 | Episodes: 100 | Success Rate: 0.05 | Mean Reward: -3.93 | Mean Length: 194.3\n",
      "Step 80,000 | Episodes: 100 | Success Rate: 0.10 | Mean Reward: -1.43 | Mean Length: 187.2\n",
      "Step 100,000 | Episodes: 100 | Success Rate: 0.00 | Mean Reward: -2.18 | Mean Length: 200.0\n",
      "Step 120,000 | Episodes: 100 | Success Rate: 0.00 | Mean Reward: -1.78 | Mean Length: 200.0\n",
      "\u001b[F\u001b[K\u001b[F\u001b[K\u001b[F\u001b[K\u001b[F\u001b[K\u001b[F\u001b[KStep 60,000 | Episodes: 100 | Success Rate: 0.05 | Mean Reward: -3.93 | Mean Length: 194.3\n",
      "Step 80,000 | Episodes: 100 | Success Rate: 0.10 | Mean Reward: -1.43 | Mean Length: 187.2\n",
      "Step 100,000 | Episodes: 100 | Success Rate: 0.00 | Mean Reward: -2.18 | Mean Length: 200.0\n",
      "Step 120,000 | Episodes: 100 | Success Rate: 0.00 | Mean Reward: -1.78 | Mean Length: 200.0\n",
      "Step 140,000 | Episodes: 100 | Success Rate: 0.15 | Mean Reward: -0.31 | Mean Length: 188.5\n",
      "\u001b[F\u001b[K\u001b[F\u001b[K\u001b[F\u001b[K\u001b[F\u001b[K\u001b[F\u001b[KStep 80,000 | Episodes: 100 | Success Rate: 0.10 | Mean Reward: -1.43 | Mean Length: 187.2\n",
      "Step 100,000 | Episodes: 100 | Success Rate: 0.00 | Mean Reward: -2.18 | Mean Length: 200.0\n",
      "Step 120,000 | Episodes: 100 | Success Rate: 0.00 | Mean Reward: -1.78 | Mean Length: 200.0\n",
      "Step 140,000 | Episodes: 100 | Success Rate: 0.15 | Mean Reward: -0.31 | Mean Length: 188.5\n",
      "Step 160,000 | Episodes: 100 | Success Rate: 0.15 | Mean Reward: -0.29 | Mean Length: 182.9\n",
      "\u001b[F\u001b[K\u001b[F\u001b[K\u001b[F\u001b[K\u001b[F\u001b[K\u001b[F\u001b[KStep 100,000 | Episodes: 100 | Success Rate: 0.00 | Mean Reward: -2.18 | Mean Length: 200.0\n",
      "Step 120,000 | Episodes: 100 | Success Rate: 0.00 | Mean Reward: -1.78 | Mean Length: 200.0\n",
      "Step 140,000 | Episodes: 100 | Success Rate: 0.15 | Mean Reward: -0.31 | Mean Length: 188.5\n",
      "Step 160,000 | Episodes: 100 | Success Rate: 0.15 | Mean Reward: -0.29 | Mean Length: 182.9\n",
      "Step 180,000 | Episodes: 100 | Success Rate: 0.00 | Mean Reward: -2.12 | Mean Length: 200.0\n",
      "\u001b[F\u001b[K\u001b[F\u001b[K\u001b[F\u001b[K\u001b[F\u001b[K\u001b[F\u001b[KStep 120,000 | Episodes: 100 | Success Rate: 0.00 | Mean Reward: -1.78 | Mean Length: 200.0\n",
      "Step 140,000 | Episodes: 100 | Success Rate: 0.15 | Mean Reward: -0.31 | Mean Length: 188.5\n",
      "Step 160,000 | Episodes: 100 | Success Rate: 0.15 | Mean Reward: -0.29 | Mean Length: 182.9\n",
      "Step 180,000 | Episodes: 100 | Success Rate: 0.00 | Mean Reward: -2.12 | Mean Length: 200.0\n",
      "Step 200,000 | Episodes: 100 | Success Rate: 0.15 | Mean Reward: -0.31 | Mean Length: 177.6\n",
      "\u001b[F\u001b[K\u001b[F\u001b[K\u001b[F\u001b[K\u001b[F\u001b[K\u001b[F\u001b[KStep 140,000 | Episodes: 100 | Success Rate: 0.15 | Mean Reward: -0.31 | Mean Length: 188.5\n",
      "Step 160,000 | Episodes: 100 | Success Rate: 0.15 | Mean Reward: -0.29 | Mean Length: 182.9\n",
      "Step 180,000 | Episodes: 100 | Success Rate: 0.00 | Mean Reward: -2.12 | Mean Length: 200.0\n",
      "Step 200,000 | Episodes: 100 | Success Rate: 0.15 | Mean Reward: -0.31 | Mean Length: 177.6\n",
      "Step 220,000 | Episodes: 100 | Success Rate: 0.10 | Mean Reward: -1.36 | Mean Length: 197.3\n",
      "\u001b[F\u001b[K\u001b[F\u001b[K\u001b[F\u001b[K\u001b[F\u001b[K\u001b[F\u001b[KStep 160,000 | Episodes: 100 | Success Rate: 0.15 | Mean Reward: -0.29 | Mean Length: 182.9\n",
      "Step 180,000 | Episodes: 100 | Success Rate: 0.00 | Mean Reward: -2.12 | Mean Length: 200.0\n",
      "Step 200,000 | Episodes: 100 | Success Rate: 0.15 | Mean Reward: -0.31 | Mean Length: 177.6\n",
      "Step 220,000 | Episodes: 100 | Success Rate: 0.10 | Mean Reward: -1.36 | Mean Length: 197.3\n",
      "Step 240,000 | Episodes: 100 | Success Rate: 0.05 | Mean Reward: -1.39 | Mean Length: 197.5\n",
      "\u001b[F\u001b[K\u001b[F\u001b[K\u001b[F\u001b[K\u001b[F\u001b[K\u001b[F\u001b[KStep 180,000 | Episodes: 100 | Success Rate: 0.00 | Mean Reward: -2.12 | Mean Length: 200.0\n",
      "Step 200,000 | Episodes: 100 | Success Rate: 0.15 | Mean Reward: -0.31 | Mean Length: 177.6\n",
      "Step 220,000 | Episodes: 100 | Success Rate: 0.10 | Mean Reward: -1.36 | Mean Length: 197.3\n",
      "Step 240,000 | Episodes: 100 | Success Rate: 0.05 | Mean Reward: -1.39 | Mean Length: 197.5\n",
      "Step 260,000 | Episodes: 100 | Success Rate: 0.10 | Mean Reward: -1.01 | Mean Length: 182.3\n",
      "\u001b[F\u001b[K\u001b[F\u001b[K\u001b[F\u001b[K\u001b[F\u001b[K\u001b[F\u001b[KStep 200,000 | Episodes: 100 | Success Rate: 0.15 | Mean Reward: -0.31 | Mean Length: 177.6\n",
      "Step 220,000 | Episodes: 100 | Success Rate: 0.10 | Mean Reward: -1.36 | Mean Length: 197.3\n",
      "Step 240,000 | Episodes: 100 | Success Rate: 0.05 | Mean Reward: -1.39 | Mean Length: 197.5\n",
      "Step 260,000 | Episodes: 100 | Success Rate: 0.10 | Mean Reward: -1.01 | Mean Length: 182.3\n",
      "Step 280,000 | Episodes: 100 | Success Rate: 0.05 | Mean Reward: -1.43 | Mean Length: 195.3\n",
      "\u001b[F\u001b[K\u001b[F\u001b[K\u001b[F\u001b[K\u001b[F\u001b[K\u001b[F\u001b[KStep 220,000 | Episodes: 100 | Success Rate: 0.10 | Mean Reward: -1.36 | Mean Length: 197.3\n",
      "Step 240,000 | Episodes: 100 | Success Rate: 0.05 | Mean Reward: -1.39 | Mean Length: 197.5\n",
      "Step 260,000 | Episodes: 100 | Success Rate: 0.10 | Mean Reward: -1.01 | Mean Length: 182.3\n",
      "Step 280,000 | Episodes: 100 | Success Rate: 0.05 | Mean Reward: -1.43 | Mean Length: 195.3\n",
      "Step 300,000 | Episodes: 100 | Success Rate: 0.10 | Mean Reward: -0.81 | Mean Length: 187.8\n",
      "\u001b[F\u001b[K\u001b[F\u001b[K\u001b[F\u001b[K\u001b[F\u001b[K\u001b[F\u001b[KStep 240,000 | Episodes: 100 | Success Rate: 0.05 | Mean Reward: -1.39 | Mean Length: 197.5\n",
      "Step 260,000 | Episodes: 100 | Success Rate: 0.10 | Mean Reward: -1.01 | Mean Length: 182.3\n",
      "Step 280,000 | Episodes: 100 | Success Rate: 0.05 | Mean Reward: -1.43 | Mean Length: 195.3\n",
      "Step 300,000 | Episodes: 100 | Success Rate: 0.10 | Mean Reward: -0.81 | Mean Length: 187.8\n",
      "Step 320,000 | Episodes: 100 | Success Rate: 0.00 | Mean Reward: -2.23 | Mean Length: 200.0\n",
      "\u001b[F\u001b[K\u001b[F\u001b[K\u001b[F\u001b[K\u001b[F\u001b[K\u001b[F\u001b[KStep 260,000 | Episodes: 100 | Success Rate: 0.10 | Mean Reward: -1.01 | Mean Length: 182.3\n",
      "Step 280,000 | Episodes: 100 | Success Rate: 0.05 | Mean Reward: -1.43 | Mean Length: 195.3\n",
      "Step 300,000 | Episodes: 100 | Success Rate: 0.10 | Mean Reward: -0.81 | Mean Length: 187.8\n",
      "Step 320,000 | Episodes: 100 | Success Rate: 0.00 | Mean Reward: -2.23 | Mean Length: 200.0\n",
      "Step 340,000 | Episodes: 100 | Success Rate: 0.05 | Mean Reward: -2.14 | Mean Length: 191.1\n",
      "\u001b[F\u001b[K\u001b[F\u001b[K\u001b[F\u001b[K\u001b[F\u001b[K\u001b[F\u001b[KStep 280,000 | Episodes: 100 | Success Rate: 0.05 | Mean Reward: -1.43 | Mean Length: 195.3\n",
      "Step 300,000 | Episodes: 100 | Success Rate: 0.10 | Mean Reward: -0.81 | Mean Length: 187.8\n",
      "Step 320,000 | Episodes: 100 | Success Rate: 0.00 | Mean Reward: -2.23 | Mean Length: 200.0\n",
      "Step 340,000 | Episodes: 100 | Success Rate: 0.05 | Mean Reward: -2.14 | Mean Length: 191.1\n",
      "Step 360,000 | Episodes: 100 | Success Rate: 0.10 | Mean Reward: -1.23 | Mean Length: 190.4\n",
      "\u001b[F\u001b[K\u001b[F\u001b[K\u001b[F\u001b[K\u001b[F\u001b[K\u001b[F\u001b[KStep 300,000 | Episodes: 100 | Success Rate: 0.10 | Mean Reward: -0.81 | Mean Length: 187.8\n",
      "Step 320,000 | Episodes: 100 | Success Rate: 0.00 | Mean Reward: -2.23 | Mean Length: 200.0\n",
      "Step 340,000 | Episodes: 100 | Success Rate: 0.05 | Mean Reward: -2.14 | Mean Length: 191.1\n",
      "Step 360,000 | Episodes: 100 | Success Rate: 0.10 | Mean Reward: -1.23 | Mean Length: 190.4\n",
      "Step 380,000 | Episodes: 100 | Success Rate: 0.05 | Mean Reward: -2.67 | Mean Length: 196.2\n",
      "\u001b[F\u001b[K\u001b[F\u001b[K\u001b[F\u001b[K\u001b[F\u001b[K\u001b[F\u001b[KStep 320,000 | Episodes: 100 | Success Rate: 0.00 | Mean Reward: -2.23 | Mean Length: 200.0\n",
      "Step 340,000 | Episodes: 100 | Success Rate: 0.05 | Mean Reward: -2.14 | Mean Length: 191.1\n",
      "Step 360,000 | Episodes: 100 | Success Rate: 0.10 | Mean Reward: -1.23 | Mean Length: 190.4\n",
      "Step 380,000 | Episodes: 100 | Success Rate: 0.05 | Mean Reward: -2.67 | Mean Length: 196.2\n",
      "Step 400,000 | Episodes: 100 | Success Rate: 0.15 | Mean Reward: -0.60 | Mean Length: 181.1\n",
      "\u001b[F\u001b[K\u001b[F\u001b[K\u001b[F\u001b[K\u001b[F\u001b[K\u001b[F\u001b[KStep 340,000 | Episodes: 100 | Success Rate: 0.05 | Mean Reward: -2.14 | Mean Length: 191.1\n",
      "Step 360,000 | Episodes: 100 | Success Rate: 0.10 | Mean Reward: -1.23 | Mean Length: 190.4\n",
      "Step 380,000 | Episodes: 100 | Success Rate: 0.05 | Mean Reward: -2.67 | Mean Length: 196.2\n",
      "Step 400,000 | Episodes: 100 | Success Rate: 0.15 | Mean Reward: -0.60 | Mean Length: 181.1\n",
      "Step 420,000 | Episodes: 100 | Success Rate: 0.10 | Mean Reward: -1.01 | Mean Length: 186.5\n",
      "\u001b[F\u001b[K\u001b[F\u001b[K\u001b[F\u001b[K\u001b[F\u001b[K\u001b[F\u001b[KStep 360,000 | Episodes: 100 | Success Rate: 0.10 | Mean Reward: -1.23 | Mean Length: 190.4\n",
      "Step 380,000 | Episodes: 100 | Success Rate: 0.05 | Mean Reward: -2.67 | Mean Length: 196.2\n",
      "Step 400,000 | Episodes: 100 | Success Rate: 0.15 | Mean Reward: -0.60 | Mean Length: 181.1\n",
      "Step 420,000 | Episodes: 100 | Success Rate: 0.10 | Mean Reward: -1.01 | Mean Length: 186.5\n",
      "Step 440,000 | Episodes: 100 | Success Rate: 0.10 | Mean Reward: -0.57 | Mean Length: 183.0\n",
      "\u001b[F\u001b[K\u001b[F\u001b[K\u001b[F\u001b[K\u001b[F\u001b[K\u001b[F\u001b[KStep 380,000 | Episodes: 100 | Success Rate: 0.05 | Mean Reward: -2.67 | Mean Length: 196.2\n",
      "Step 400,000 | Episodes: 100 | Success Rate: 0.15 | Mean Reward: -0.60 | Mean Length: 181.1\n",
      "Step 420,000 | Episodes: 100 | Success Rate: 0.10 | Mean Reward: -1.01 | Mean Length: 186.5\n",
      "Step 440,000 | Episodes: 100 | Success Rate: 0.10 | Mean Reward: -0.57 | Mean Length: 183.0\n",
      "Step 460,000 | Episodes: 100 | Success Rate: 0.05 | Mean Reward: -1.77 | Mean Length: 192.7\n",
      "\u001b[F\u001b[K\u001b[F\u001b[K\u001b[F\u001b[K\u001b[F\u001b[K\u001b[F\u001b[KStep 400,000 | Episodes: 100 | Success Rate: 0.15 | Mean Reward: -0.60 | Mean Length: 181.1\n",
      "Step 420,000 | Episodes: 100 | Success Rate: 0.10 | Mean Reward: -1.01 | Mean Length: 186.5\n",
      "Step 440,000 | Episodes: 100 | Success Rate: 0.10 | Mean Reward: -0.57 | Mean Length: 183.0\n",
      "Step 460,000 | Episodes: 100 | Success Rate: 0.05 | Mean Reward: -1.77 | Mean Length: 192.7\n",
      "Step 480,000 | Episodes: 100 | Success Rate: 0.05 | Mean Reward: -1.50 | Mean Length: 192.4\n",
      "\u001b[F\u001b[K\u001b[F\u001b[K\u001b[F\u001b[K\u001b[F\u001b[K\u001b[F\u001b[KStep 420,000 | Episodes: 100 | Success Rate: 0.10 | Mean Reward: -1.01 | Mean Length: 186.5\n",
      "Step 440,000 | Episodes: 100 | Success Rate: 0.10 | Mean Reward: -0.57 | Mean Length: 183.0\n",
      "Step 460,000 | Episodes: 100 | Success Rate: 0.05 | Mean Reward: -1.77 | Mean Length: 192.7\n",
      "Step 480,000 | Episodes: 100 | Success Rate: 0.05 | Mean Reward: -1.50 | Mean Length: 192.4\n",
      "Step 500,000 | Episodes: 100 | Success Rate: 0.15 | Mean Reward: -1.14 | Mean Length: 185.1\n",
      "\n",
      "âœ… Training completed and model saved!\n"
     ]
    }
   ],
   "source": [
    "import torch as th\n",
    "import torch.nn as nn\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "\n",
    "class MazeCNN(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space, features_dim: int = 512):\n",
    "        super().__init__(observation_space, features_dim)\n",
    "        n_ch, H, W = observation_space.shape\n",
    "        \n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(n_ch, 64, 3, 1, 1), nn.BatchNorm2d(64), nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, 3, 1, 1), nn.BatchNorm2d(128), nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, 3, 1, 1), nn.BatchNorm2d(256), nn.ReLU(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        \n",
    "        with th.no_grad():\n",
    "            n_flat = self.cnn(th.zeros(1, n_ch, H, W)).shape[1]\n",
    "        \n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(n_flat, features_dim), nn.ReLU(), nn.Dropout(0.1),\n",
    "            nn.Linear(features_dim, features_dim), nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(self.cnn(x))\n",
    "\n",
    "n_envs = vec_env.num_envs\n",
    "n_steps = 512  # Original value\n",
    "buffer_size = n_steps * n_envs\n",
    "\n",
    "logdir = \"tb_logs/ppo_maze\"\n",
    "\n",
    "model = PPO(\n",
    "    policy=\"CnnLstmPolicy\",  # LSTM for memory in large mazes\n",
    "    env=vec_env,\n",
    "    n_steps=1024,           # More steps for complex environments\n",
    "    batch_size=512,         # Larger batches\n",
    "    learning_rate=3e-4,     # Higher LR for complex learning\n",
    "    gamma=0.995,            # Longer planning horizon\n",
    "    ent_coef=0.01,          # Balanced exploration\n",
    "    policy_kwargs=dict(\n",
    "        features_extractor_class=MazeCNN,\n",
    "        features_extractor_kwargs=dict(features_dim=512),\n",
    "        lstm_hidden_size=256,  # Memory for navigation\n",
    "        n_lstm_layers=1,\n",
    "        normalize_images=False,\n",
    "    ),\n",
    "    verbose=0,\n",
    "    tensorboard_log=\"tb_logs/large_maze\",\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")\n",
    "\n",
    "total_timesteps = 2_000_000  # Much more training needed\n",
    "\n",
    "class LargeMazeCallback(BaseCallback):\n",
    "    def _on_step(self):\n",
    "        if self.n_calls % 10000 == 0:  # Less frequent updates\n",
    "            if self.model.ep_info_buffer:\n",
    "                recent = list(self.model.ep_info_buffer)[-50:]\n",
    "                successes = sum(1 for ep in recent if ep.get('r', 0) > 5)\n",
    "                success_rate = successes / len(recent) if recent else 0\n",
    "                mean_reward = np.mean([ep['r'] for ep in recent])\n",
    "                mean_length = np.mean([ep['l'] for ep in recent])\n",
    "                \n",
    "                print(f\"Step {self.num_timesteps:,} | Success: {success_rate:.2f} | \"\n",
    "                      f\"Reward: {mean_reward:.2f} | Length: {mean_length:.1f}\")\n",
    "        return True\n",
    "\n",
    "callback = LargeMazeCallback()\n",
    "model.learn(total_timesteps=total_timesteps, callback=callback)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3819e0f2",
   "metadata": {},
   "source": [
    "\n",
    "## 6. Evaluate on held-out mazes\n",
    "We measure:\n",
    "- Success rate\n",
    "- Average steps\n",
    "- Ratio of steps to A* shortest path (lower is better; target â‰¤ 1.5Ã—)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bd2175af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating trained model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval 15x15: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:18<00:00,  2.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: Success Rate: 0.000, Avg Steps: 200.0, Steps/A* Ratio: N/A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def evaluate_with_astar(model, episodes=200, size=15):\n",
    "    \"\"\"Evaluate the model and compare performance to A* shortest path.\"\"\"\n",
    "    successes, total_steps, ratios = 0, 0, []\n",
    "    \n",
    "    for _ in trange(episodes, desc=f\"Eval {size}x{size}\"):\n",
    "        env = MazeEnv(size=size, view=7, max_steps=200, seed=np.random.randint(1e9))\n",
    "        obs, _ = env.reset()\n",
    "        start = env.agent\n",
    "        goal = env.goal\n",
    "        \n",
    "        # Calculate A* optimal path length\n",
    "        L_astar = astar_length(env.grid, start, goal)\n",
    "        \n",
    "        # Run episode with the model\n",
    "        steps = 0\n",
    "        with torch.no_grad():\n",
    "            while True:\n",
    "                action, _ = model.predict(obs, deterministic=True)\n",
    "                obs, reward, terminated, truncated, info = env.step(action)\n",
    "                steps += 1\n",
    "                if terminated or truncated:\n",
    "                    break\n",
    "        \n",
    "        total_steps += steps\n",
    "        if env.agent == env.goal:  # Success\n",
    "            successes += 1\n",
    "            if L_astar is not None and L_astar > 0:\n",
    "                ratios.append(steps / L_astar)\n",
    "    \n",
    "    sr = successes / episodes\n",
    "    avg_steps = total_steps / episodes\n",
    "    avg_ratio = float(np.mean(ratios)) if ratios else None\n",
    "    return sr, avg_steps, avg_ratio\n",
    "\n",
    "# Run evaluation after training\n",
    "print(\"Evaluating trained model...\")\n",
    "sr, avg_steps, avg_ratio = evaluate_with_astar(model, episodes=50, size=15)\n",
    "print(f\"Results: Success Rate: {sr:.3f}, Avg Steps: {avg_steps:.1f}, Steps/A* Ratio: {avg_ratio:.2f}\" if avg_ratio else f\"Results: Success Rate: {sr:.3f}, Avg Steps: {avg_steps:.1f}, Steps/A* Ratio: N/A\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3621159c",
   "metadata": {},
   "source": [
    "\n",
    "## 7. Record a few rollouts to GIF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "286a25aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating demo video...\n",
      "âœ… Saved videos/ppo_maze_demo.gif\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def render_episode(model, size=15, max_steps=500):\n",
    "    \"\"\"Render a complete episode and return frames for GIF creation.\"\"\"\n",
    "    env = MazeEnv(size=size, view=7, max_steps=max_steps, seed=np.random.randint(1e9))\n",
    "    frames = []\n",
    "    obs, _ = env.reset()\n",
    "    frames.append(env.render())\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_steps):\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            obs, reward, terminated, truncated, info = env.step(action)\n",
    "            frames.append(env.render())\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "    \n",
    "    return frames\n",
    "\n",
    "# Create video directory and generate demo\n",
    "os.makedirs(\"videos\", exist_ok=True)\n",
    "print(\"Generating demo video...\")\n",
    "frames = render_episode(model, size=10)\n",
    "imageio.mimsave(\"videos/ppo_maze_demo.gif\", frames, duration=0.08)\n",
    "print(\"âœ… Saved videos/ppo_maze_demo.gif\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa871c01",
   "metadata": {},
   "source": [
    "\n",
    "## 8. (Optional) Try an LSTM policy (helps with partial observability)\n",
    "Re-train with `MlpLstmPolicy` or `CnnLstmPolicy` for better memory at junctions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc9e14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example (commented out by default):\n",
    "# model_lstm = PPO(\n",
    "#     policy=\"CnnLstmPolicy\",\n",
    "#     env=vec_env,\n",
    "#     n_steps=256,\n",
    "#     batch_size=4096,\n",
    "#     learning_rate=3e-4,\n",
    "#     gamma=0.995,\n",
    "#     gae_lambda=0.95,\n",
    "#     clip_range=0.2,\n",
    "#     vf_coef=0.5,\n",
    "#     ent_coef=0.01,\n",
    "#     verbose=1,\n",
    "#     tensorboard_log=\"tb_logs/ppo_maze_lstm\",\n",
    "#     device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "# )\n",
    "# model_lstm.learn(total_timesteps=1_000_000)\n",
    "# model_lstm.save(\"ppo_maze_partial_lstm\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db4d916",
   "metadata": {},
   "source": [
    "\n",
    "## 9. TensorBoard\n",
    "In a separate terminal:\n",
    "```bash\n",
    "tensorboard --logdir tb_logs\n",
    "```\n",
    "Then open the URL it prints (usually http://localhost:6006).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
