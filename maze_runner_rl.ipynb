{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c31d15a",
   "metadata": {},
   "source": [
    "\n",
    "# üß≠ Maze Runner RL (PPO + PyTorch + Stable-Baselines3)\n",
    "\n",
    "This notebook trains an agent with **PPO** to navigate **procedurally generated mazes** using **partial observations**.\n",
    "It is GPU-ready and logs to TensorBoard. Works on Linux/macOS/Windows (WSL recommended on Windows).\n",
    "\n",
    "**What you'll get**\n",
    "- A custom `gymnasium` environment (`MazeEnv`) with egocentric 7√ó7 view\n",
    "- Procedural maze generator (DFS backtracker)\n",
    "- A* shortest-path baseline for evaluation\n",
    "- PPO training (Stable-Baselines3) with CNN policy\n",
    "- Metrics: success rate, average steps, steps vs A* ratio\n",
    "- Optional LSTM policy (partial observability)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041d6452",
   "metadata": {},
   "source": [
    "\n",
    "## 0. Environment setup (run once in your local machine)\n",
    "> **Tip:** Install the correct CUDA-enabled torch first. Pick one of the commands below that matches your system.\n",
    "\n",
    "**Conda (recommended):**\n",
    "```bash\n",
    "# Create env\n",
    "conda create -n maze-rl python=3.11 -y\n",
    "conda activate maze-rl\n",
    "\n",
    "# Install PyTorch with CUDA (choose the right cudatoolkit)\n",
    "# See https://pytorch.org/get-started/locally/ for exact command for your system\n",
    "# Example for CUDA 12.x:\n",
    "pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "# Core libs\n",
    "pip install gymnasium stable-baselines3[extra] tensorboard moviepy imageio imageio[ffmpeg] pygame\n",
    "pip install matplotlib numpy tqdm mlflow  # optional extras\n",
    "```\n",
    "\n",
    "**PyTorch CPU-only (if no NVIDIA GPU):**\n",
    "```bash\n",
    "pip install torch torchvision --index-url https://download.pytorch.org/whl/cpu\n",
    "```\n",
    "Then, start Jupyter:\n",
    "```bash\n",
    "jupyter lab  # or jupyter notebook\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "331a4bdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.13.2 | OS: Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.31\n",
      "PyTorch: 2.8.0+cu128\n",
      "CUDA available: True\n",
      "GPU name: NVIDIA GeForce RTX 2070\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# üîé Quick GPU sanity check\n",
    "import torch, platform, sys\n",
    "print(\"Python:\", sys.version.split()[0], \"| OS:\", platform.platform())\n",
    "print(\"PyTorch:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU name:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"NOTE: CUDA not available. Training will run on CPU (much slower).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f3be9305",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Core imports\n",
    "import numpy as np\n",
    "import math, random\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple, List, Optional\n",
    "\n",
    "# Gymnasium\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "# RL\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv, DummyVecEnv\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.callbacks import BaseCallback, EvalCallback\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "# Utils\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio\n",
    "from tqdm import trange\n",
    "import os, time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d689e262",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Maze generation (DFS backtracker)\n",
    "Generates a *perfect* maze (single path between any two points) on odd-sized grids.\n",
    "`1 = wall`, `0 = free space`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "671d7c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_maze(width: int, height: int, rng: np.random.Generator) -> np.ndarray:\n",
    "    \"\"\"Generate a perfect maze with DFS backtracking.\n",
    "    Grid is odd x odd, walls=1, free=0.\"\"\"\n",
    "    w = width if width % 2 == 1 else width + 1\n",
    "    h = height if height % 2 == 1 else height + 1\n",
    "    grid = np.ones((h, w), dtype=np.int8)\n",
    "\n",
    "    def neighbors(r, c):\n",
    "        for dr, dc in [(-2,0),(2,0),(0,-2),(0,2)]:\n",
    "            nr, nc = r+dr, c+dc\n",
    "            if 1 <= nr < h-1 and 1 <= nc < w-1:\n",
    "                yield nr, nc, r+dr//2, c+dc//2  # cell and the wall between\n",
    "\n",
    "    # start at random odd cell\n",
    "    start_r = rng.integers(1, h-1)\n",
    "    start_c = rng.integers(1, w-1)\n",
    "    start_r -= (start_r % 2 == 0)\n",
    "    start_c -= (start_c % 2 == 0)\n",
    "    stack = [(start_r, start_c)]\n",
    "    grid[start_r, start_c] = 0\n",
    "\n",
    "    while stack:\n",
    "        r, c = stack[-1]\n",
    "        nbrs = [(nr, nc, wr, wc) for nr, nc, wr, wc in neighbors(r, c) if grid[nr, nc] == 1]\n",
    "        if nbrs:\n",
    "            nr, nc, wr, wc = nbrs[rng.integers(len(nbrs))]\n",
    "            grid[wr, wc] = 0\n",
    "            grid[nr, nc] = 0\n",
    "            stack.append((nr, nc))\n",
    "        else:\n",
    "            stack.pop()\n",
    "\n",
    "    return grid\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef098fa6",
   "metadata": {},
   "source": [
    "\n",
    "## 2. A* shortest-path baseline\n",
    "Used for evaluation (not available to the agent).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "32e0f1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "import torch.nn as nn\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "\n",
    "class MazeCNN(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space, features_dim: int = 512):\n",
    "        super().__init__(observation_space, features_dim)\n",
    "        n_ch, H, W = observation_space.shape  # (4,7,7)\n",
    "        \n",
    "        # Deeper CNN for better spatial understanding in large mazes\n",
    "        self.cnn = nn.Sequential(\n",
    "            # First block - capture local features\n",
    "            nn.Conv2d(n_ch, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64), nn.ReLU(inplace=True),\n",
    "            \n",
    "            # Second block - spatial patterns\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128), nn.ReLU(inplace=True),\n",
    "            \n",
    "            # Third block - complex spatial reasoning\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256), nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Flatten()\n",
    "        )\n",
    "        \n",
    "        with th.no_grad():\n",
    "            n_flat = self.cnn(th.zeros(1, n_ch, H, W)).shape[1]\n",
    "        \n",
    "        # Multi-layer processing for complex decision making\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(n_flat, features_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(features_dim, features_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(features_dim, features_dim),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(self.cnn(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "0de51c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import heapq\n",
    "\n",
    "def astar_length(grid: np.ndarray, start: Tuple[int,int], goal: Tuple[int,int]) -> Optional[int]:\n",
    "    H, W = grid.shape\n",
    "    def h(p):  # Manhattan heuristic\n",
    "        return abs(p[0]-goal[0]) + abs(p[1]-goal[1])\n",
    "    open_set = [(h(start), 0, start)]\n",
    "    g = {start: 0}\n",
    "    visited = set()\n",
    "\n",
    "    while open_set:\n",
    "        _, cost, node = heapq.heappop(open_set)\n",
    "        if node in visited:\n",
    "            continue\n",
    "        visited.add(node)\n",
    "        if node == goal:\n",
    "            return cost\n",
    "        r, c = node\n",
    "        for dr, dc in [(-1,0),(1,0),(0,-1),(0,1)]:\n",
    "            nr, nc = r+dr, c+dc\n",
    "            if 0 <= nr < H and 0 <= nc < W and grid[nr, nc] == 0:\n",
    "                new_cost = cost + 1\n",
    "                if new_cost < g.get((nr, nc), 1e9):\n",
    "                    g[(nr, nc)] = new_cost\n",
    "                    heapq.heappush(open_set, (new_cost + h((nr, nc)), new_cost, (nr, nc)))\n",
    "    return None  # unreachable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91aea83b",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Gymnasium Environment (`MazeEnv`)\n",
    "- **Observation:** egocentric 7√ó7 crop with 4 channels (empty, wall, goal, agent)\n",
    "- **Actions:** 4-way movement\n",
    "- **Rewards:** +1 for goal, -0.01 per step, -0.05 on wall bump, optional shaping toward goal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "075154e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MazeEnv(gym.Env):\n",
    "    metadata = {\"render_modes\": [\"rgb_array\"]}\n",
    "\n",
    "    def __init__(self, size=15, view=7, max_steps=200, seed=None, shaped=True):\n",
    "        super().__init__()\n",
    "        self.size = size\n",
    "        self.view = view\n",
    "        self.max_steps = max_steps\n",
    "        self.shaped = shaped\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "\n",
    "        self.action_space = spaces.Discrete(4)  # U,D,L,R\n",
    "        # 4 channels: empty, wall, goal, agent\n",
    "        self.observation_space = spaces.Box(low=0.0, high=1.0, shape=(4, view, view), dtype=np.float32)\n",
    "\n",
    "        self.grid = None\n",
    "        self.agent = None\n",
    "        self.goal = None\n",
    "        self.steps = 0\n",
    "\n",
    "    def _reset_level(self):\n",
    "        self.grid = generate_maze(self.size, self.size, self.rng)\n",
    "        # pick start/goal on free cells, far apart\n",
    "        free = np.argwhere(self.grid == 0)\n",
    "        a_idx = self.rng.integers(len(free))\n",
    "        g_idx = self.rng.integers(len(free))\n",
    "        self.agent = tuple(free[a_idx])\n",
    "        self.goal = tuple(free[g_idx])\n",
    "        # ensure reachable and reasonably far\n",
    "        for _ in range(200):\n",
    "            L = astar_length(self.grid, self.agent, self.goal)\n",
    "            if L is not None and L > self.size//2:\n",
    "                break\n",
    "            a_idx = self.rng.integers(len(free))\n",
    "            g_idx = self.rng.integers(len(free))\n",
    "            self.agent = tuple(free[a_idx])\n",
    "            self.goal = tuple(free[g_idx])\n",
    "\n",
    "    def _obs(self):\n",
    "        # egocentric crop centered at agent, padded with walls\n",
    "        pad = self.view//2\n",
    "        H, W = self.grid.shape\n",
    "        padded = np.ones((H+2*pad, W+2*pad), dtype=np.int8)\n",
    "        padded[pad:pad+H, pad:pad+W] = self.grid\n",
    "\n",
    "        ar, ac = self.agent\n",
    "        gr, gc = self.goal\n",
    "        crop = padded[ar:ar+2*pad+1, ac:ac+2*pad+1]\n",
    "\n",
    "        empty = (crop == 0).astype(np.float32)\n",
    "        wall  = (crop == 1).astype(np.float32)\n",
    "\n",
    "        goal_layer = np.zeros_like(empty, dtype=np.float32)\n",
    "        ga, gb = pad+(gr-ar), pad+(gc-ac)\n",
    "        if 0 <= ga < self.view and 0 <= gb < self.view:\n",
    "            goal_layer[ga, gb] = 1.0\n",
    "\n",
    "        agent_layer = np.zeros_like(empty, dtype=np.float32)\n",
    "        agent_layer[pad, pad] = 1.0\n",
    "\n",
    "        obs = np.stack([empty, wall, goal_layer, agent_layer], axis=0)\n",
    "        return obs\n",
    "\n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        if seed is not None:\n",
    "            self.rng = np.random.default_rng(seed)\n",
    "        self._reset_level()\n",
    "        self.steps = 0\n",
    "        obs = self._obs()\n",
    "        info = {}\n",
    "        return obs, info\n",
    "\n",
    "    def step(self, action):\n",
    "        self.steps += 1\n",
    "        r, c = self.agent\n",
    "        drdc = [(-1,0),(1,0),(0,-1),(0,1)][int(action)]\n",
    "        nr, nc = r+drdc[0], c+drdc[1]\n",
    "\n",
    "        reward = -0.005  # Reduced step penalty for large mazes\n",
    "        done = False\n",
    "\n",
    "        if 0 <= nr < self.grid.shape[0] and 0 <= nc < self.grid.shape[1] and self.grid[nr, nc] == 0:\n",
    "            # valid move\n",
    "            old_dist = abs(r-self.goal[0]) + abs(c-self.goal[1])\n",
    "            new_dist = abs(nr-self.goal[0]) + abs(nc-self.goal[1])\n",
    "            \n",
    "            # Enhanced reward shaping for large mazes\n",
    "            if self.shaped:\n",
    "                # Strong directional rewards\n",
    "                if new_dist < old_dist:\n",
    "                    reward += 0.05  # Strong reward for getting closer\n",
    "                elif new_dist > old_dist:\n",
    "                    reward -= 0.01  # Penalty for moving away\n",
    "                \n",
    "                # Distance-based rewards for large mazes\n",
    "                max_dist = self.size * 1.4  # Diagonal distance\n",
    "                distance_reward = 0.02 * (1.0 - new_dist / max_dist)\n",
    "                reward += distance_reward\n",
    "                \n",
    "                # Proximity bonus - extra reward when very close to goal\n",
    "                if new_dist <= 3:\n",
    "                    reward += 0.1  # Strong bonus when very close\n",
    "                elif new_dist <= 7:\n",
    "                    reward += 0.05  # Medium bonus when close\n",
    "            \n",
    "            self.agent = (nr, nc)\n",
    "        else:\n",
    "            # wall bump - stronger penalty to discourage this behavior\n",
    "            reward -= 0.2\n",
    "\n",
    "        if self.agent == self.goal:\n",
    "            reward += 50.0  # Very large reward for reaching goal in large mazes\n",
    "            done = True\n",
    "\n",
    "        truncated = self.steps >= self.max_steps\n",
    "        obs = self._obs()\n",
    "        info = {}\n",
    "        return obs, reward, done, truncated, info\n",
    "\n",
    "    def render(self):\n",
    "        # simple top-down RGB\n",
    "        H, W = self.grid.shape\n",
    "        img = np.zeros((H, W, 3), dtype=np.uint8)\n",
    "        img[self.grid == 1] = (30, 30, 30)    # walls\n",
    "        img[self.grid == 0] = (220, 220, 220) # floor\n",
    "        gr, gc = self.goal\n",
    "        img[gr, gc] = (50, 190, 60)           # goal (green)\n",
    "        ar, ac = self.agent\n",
    "        img[ar, ac] = (30, 144, 255)          # agent (blue)\n",
    "        return img\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9d75d1",
   "metadata": {},
   "source": [
    "\n",
    "### Quick visual sanity check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1a247335",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAcoAAAGbCAYAAABETtCOAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjYsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvq6yFwwAAAAlwSFlzAAAPYQAAD2EBqD+naQAAJZpJREFUeJzt3Xl4VPWh//HPJCEJJhDZlwhBAt6wRNGAbLJ0EEIv+xYELSQiyxXoRa3eqtdG6gYUFSki4hJLFWW3iGKEToAUqAWESGmpQAJesSVAIIAsgeT7+4PfnGYyyTcTBAP4fj0Pz2NOzsz5njMnec9ZJrqMMUYAAKBUQZU9AAAArmaEEgAAC0IJAIAFoQQAwIJQAgBgQSgBALAglAAAWBBKAAAsCCUAABY/qlC6XC49/fTTlT2Mq85f/vIXhYaG6sCBA860Jk2aqG/fvuU+dt26dXK5XFq3bt0VHCFw9WnSpImSk5Mv63N2795d3bt3v6zP6f0ZXbp06WV93h9KyW2yf/9+uVwuvfPOOxV6ng4dOuixxx67pDFUOJQ7d+7U0KFDFRMTo/DwcEVHR6tnz5767W9/e0kDuJY1adJELpdLd999d6nff+ONN+RyueRyubR169YfeHSBe/LJJzVixAjFxMRU9lB+lBYuXKhZs2ZV9jCA69r//M//6NVXX9W//vWvCj+2QqHctGmT2rZtq6ysLI0dO1Zz5szRAw88oKCgIL3yyisVXvj1IDw8XBkZGaVu/Pfee0/h4eGVMKrA7dixQ2vXrtWECRMqeyg/WoQSuPIGDBig6tWra+7cuRV+bEhFZn7uuecUFRWlLVu26MYbb/T5Xm5uboUXfj3o3LmztmzZokWLFum///u/nenffPONMjMzNWjQIC1btqwSR2iXlpamxo0bq0OHDpU9FFSSCxcuqKioSKGhoZU9FFxG3tcVFwUFBWno0KFasGCBpk6dKpfLFfhjK7Kgffv2qVWrVn6RlKS6dev6fJ2Wlia32626desqLCxMLVu21Guvveb3OO+1sHXr1qlt27aqWrWq4uPjnWtey5cvV3x8vMLDw5WQkKDt27f7PD45OVmRkZHKzs5WYmKiIiIi1LBhQ/36179WIP9jlIMHD+r+++9XvXr1FBYWplatWuntt98OeJuEh4dr8ODBWrhwoc/0999/XzVq1FBiYqLfY7788kslJyeradOmCg8PV/369XX//ffr6NGjzjze8/Bl/Svu888/V+/evRUVFaUbbrhB3bp108aNGwMa/4cffii3213mTvPZZ5+pTZs2Cg8PV8uWLbV8+fJyn7OsazelXX85d+6cUlNT1axZM4WFhalRo0Z67LHHdO7cuYDGX56ZM2eqU6dOqlWrlqpWraqEhIRSr9WcOXNGP//5z1W7dm1Vq1ZN/fv318GDB0u9rh3IPuO9LrR48WI999xzuummmxQeHq4ePXpo7969Ptvk448/1oEDB5zXtkmTJpe0rkuWLFHLli0VHh6u1q1ba8WKFUpOTvZ5Pu9+NXPmTM2aNUuxsbEKCwvT3/72N0nS7t27NXToUNWsWVPh4eFq27atVq5c6bes48ePa8qUKWrUqJHCwsLUrFkzTZ8+3ecXc/FlzZ8/31lWu3bttGXLlktax9K8+uqratq0qapWrao777xTmZmZpe5rubm5GjNmjOrVq6fw8HDddttt+t3vfuf3fIHuM9+Xd5sUH3dJBQUF+tWvfqWEhARFRUUpIiJCXbp0UUZGhs985b2uJZ07d059+/ZVVFSUNm3adMnrsHLlSrlcLn355ZfOtGXLlsnlcmnw4ME+87Zo0ULDhw93vg60EYH417/+pZSUFN10000KCwtTgwYNNGDAAO3fv99nvp49e+rAgQPasWNHhZ6/QkeUMTEx2rx5s/7617+qdevW1nlfe+01tWrVSv3791dISIg++ugjPfjggyoqKtLEiRN95t27d69Gjhyp8ePH67777tPMmTPVr18/zZs3T0888YQefPBBSdILL7ygpKQk/eMf/1BQ0L8bX1hYqN69e6tDhw6aMWOGPv30U6WmpurChQv69a9/XeYYDx06pA4dOsjlcmnSpEmqU6eOVq9erTFjxujEiROaMmVKQNtl5MiR6tWrl/bt26fY2FhJF0+nDR06VFWqVPGbf82aNcrOzlZKSorq16+vXbt2af78+dq1a5f+/Oc/y+VyqU6dOvr973/v87jz58/roYce8nnn7/F49NOf/lQJCQlKTU1VUFCQswNmZmbqzjvvLHPcBw8e1Ndff6077rij1O/v2bNHw4cP14QJEzR69GilpaVp2LBh+vTTT9WzZ8+Ato1NUVGR+vfvrz/96U8aN26cWrRooZ07d+rll1/WV199pQ8//NCZNz8/X+fPny/3OcPDwxUZGel8/corr6h///669957VVBQoA8++EDDhg3TqlWr1KdPH2e+5ORkLV68WD/72c/UoUMHrV+/3uf7XhXdZ6ZNm6agoCD94he/UH5+vmbMmKF7771Xn3/+uaSL14fz8/P1zTff6OWXX5Ykn/EH6uOPP9bw4cMVHx+vF154QceOHdOYMWMUHR1d6vxpaWk6e/asxo0bp7CwMNWsWVO7du1S586dFR0drV/+8peKiIjQ4sWLNXDgQC1btkyDBg2SJJ0+fVrdunXTwYMHNX78eDVu3FibNm3S448/rn/+859+p5EXLlyokydPavz48XK5XJoxY4YGDx6s7Oxs5+fj3LlzOnnyZEDrWrt2bee/X3vtNU2aNEldunTRQw89pP3792vgwIGqUaOGbrrpJme+M2fOqHv37tq7d68mTZqkm2++WUuWLFFycrKOHz/uczYo0H3m+3jrrbc0fvx4derUSVOmTFF2drb69++vmjVrqlGjRs58J06c0JtvvqkRI0Zo7NixOnnypN566y0lJibqL3/5i9q0aePzvKW9rsePH/eZ58yZMxowYIC2bt2qtWvXql27dpIu/n7Jz88PaPw1a9ZUUFCQ7rrrLrlcLm3YsEG33nqrJCkzM1NBQUH605/+5Mx/+PBh7d69W5MmTXKmVaQR5RkyZIh27dqlyZMnq0mTJsrNzdWaNWv09ddf+7xRTEhIkCRt3LhRt99+e+ALMBXw2WefmeDgYBMcHGw6duxoHnvsMZOenm4KCgr85j19+rTftMTERNO0aVOfaTExMUaS2bRpkzMtPT3dSDJVq1Y1Bw4ccKa//vrrRpLJyMhwpo0ePdpIMpMnT3amFRUVmT59+pjQ0FBz+PBhZ7okk5qa6nw9ZswY06BBA3PkyBGfMd1zzz0mKiqq1HUoOfY+ffqYCxcumPr165tnnnnGGGPM3/72NyPJrF+/3qSlpRlJZsuWLdZt8/777xtJZsOGDWUu78EHHzTBwcHG4/E469m8eXOTmJhoioqKfJ7/5ptvNj179rSOf+3atUaS+eijj0pdN0lm2bJlzrT8/HzToEEDc/vttzvTMjIy/F6TmJgYM3r0aL/n7Natm+nWrZvz9e9//3sTFBRkMjMzfeabN2+ekWQ2btzo81hJ5f4rudyS27qgoMC0bt3auN1uZ9q2bduMJDNlyhSfeZOTky95n/FulxYtWphz5845873yyitGktm5c6czrU+fPiYmJsZve1VEfHy8uemmm8zJkyedaevWrTOSfJ47JyfHSDLVq1c3ubm5Ps/Ro0cPEx8fb86ePetMKyoqMp06dTLNmzd3pj3zzDMmIiLCfPXVVz6P/+Uvf2mCg4PN119/7bOsWrVqmby8PGe+P/zhD377nffnJJB/XufOnTO1atUy7dq1M+fPn3emv/POO0aSz742a9YsI8m8++67zrSCggLTsWNHExkZaU6cOOFMD2SfMabs/bw8BQUFpm7duqZNmzY++8b8+fP9xn3hwgWfeYwx5tixY6ZevXrm/vvvd6bZXlfvvrhkyRJz8uRJ061bN1O7dm2zffv2UucL5F9OTo7zuFatWpmkpCTn6zvuuMMMGzbMSDJ///vfjTHGLF++3EgyWVlZznyBNqLk7w3vuqalpTnbQ5L5zW9+4/d8pQkNDTX/9V//FdC8XhU69dqzZ09t3rxZ/fv3V1ZWlmbMmKHExERFR0f7nZ6pWrWq89/5+fk6cuSIunXrpuzsbL93LS1btlTHjh2dr9u3by9Jcrvdaty4sd/07Oxsv7EVf6fifbdfUFCgtWvXlrouxhgtW7ZM/fr1kzFGR44ccf4lJiYqPz9fX3zxRUDbJTg4WElJSXr//fclXbyJp1GjRurSpUup8xffNmfPntWRI0eca4RlLXPBggWaO3euZsyYoZ/85CeSLt6Is2fPHo0cOVJHjx51xv/dd9+pR48e2rBhg/UahfdUb40aNUr9fsOGDZ2jCEmqXr26Ro0ape3bt1/SnWMlLVmyRC1atFBcXJzP9ne73ZLkc3rpxRdf1Jo1a8r9V/L27+Lb+tixY8rPz1eXLl18tvOnn34qSc6ZC6/Jkyf7fH0p+0xKSorPGQDvPlHaPnypvv32W+3cuVOjRo3yORrt1q2b4uPjS33MkCFDVKdOHefrvLw8eTweJSUl6eTJk856HT16VImJidqzZ48OHjwo6eLr1qVLF9WoUcNnG9x9990qLCzUhg0bfJY1fPhwn32stG2QmJgY0Ou7Zs0a5zFbt27V0aNHNXbsWIWE/Pvk2L333uu3T3/yySeqX7++RowY4UyrUqWKfv7zn+vUqVNav369Mz2Qfeb72Lp1q3JzczVhwgSffSM5OVlRUVE+8wYHBzvzFBUVKS8vTxcuXFDbtm1LHU/J17W4/Px89erVS7t379a6dev8jkZvu+22gF+D+vXrO4/r0qWLc9r45MmTysrK0rhx41S7dm1nemZmpm688UafM5EVaYRN1apVFRoaqnXr1unYsWPlzu/dbyuiQqdeJaldu3Zavny5CgoKlJWVpRUrVujll1/W0KFDtWPHDrVs2VLSxUPb1NRUbd68WadPn/Z5jvz8fJ8dongMJTnfK34Kovj0khsjKChITZs29Zl2yy23SJLfOWqvw4cP6/jx45o/f77mz59f6jwVuUFp5MiRmj17trKysrRw4ULdc889ZV73y8vL09SpU/XBBx/4LaO0HWTHjh2aMGGCRowYoYcfftiZvmfPHknS6NGjyxxXfn5+mSH0MmVcy23WrJnfOhTfrsV/WC7Fnj179Pe//73MH+zi28Z7yqSiVq1apWeffVY7duzwue5ZfL0OHDigoKAg3XzzzT6Pbdasmc/Xl7LPlNy3va9FID/QgfJ+/rXkeL3TSvuFWnJd9+7dK2OMnnrqKT311FOlLic3N1fR0dHas2ePvvzyy4BeNymwbdCgQQM1aNCg1OcrS1nrHRIS4ned98CBA2revLnPJRvp4nWz4s8lBbbPfB/eZTVv3txnepUqVfx+j0nS7373O7344ovavXu3z+WHkq9hWdO8pkyZorNnz2r79u1q1aqV3/dr1KhR5kfdbLp06aJ58+Zp79692rdvn1wulzp27OgEdOzYscrMzFTnzp19tn9FGmETFham6dOn65FHHlG9evXUoUMH9e3bV6NGjSr1d5QxpsKvZYVD6RUaGqp27dqpXbt2uuWWW5SSkqIlS5YoNTVV+/btU48ePRQXF6eXXnpJjRo1UmhoqD755BO9/PLLfkc5wcHBpS6jrOll/WKvCO8Y7rvvvjJD4z3nHoj27dsrNjZWU6ZMUU5OjkaOHFnmvElJSdq0aZMeffRRtWnTRpGRkSoqKlLv3r39ts2xY8c0ZMgQ3XLLLXrzzTdLXYff/OY3fu8OvWzXu2rVquUs43IqaycsLCz0eU2LiooUHx+vl156qdT5i79RysvLU0FBQbnLrlq1qvMDlpmZqf79+6tr166aO3euGjRooCpVqigtLc3v5qtAXMo+cyX34e+j+Lt56d/r9otf/KLUG9CkfwepqKhIPXv2LPPD2943U16BbIMzZ84EfBTxfd+g2Vzufeb7evfdd5WcnKyBAwfq0UcfVd26dRUcHKwXXnhB+/bt85u/5Ota3IABA/TBBx9o2rRpWrBggd+bhoKCAuXl5QU0rjp16jiv61133SVJ2rBhg7Kzs3XHHXc4Nx3Nnj1bp06d0vbt2/Xcc885j69oI8ozZcoU9evXTx9++KHS09P11FNP6YUXXpDH4/G7Fnn8+HGf69yBuORQFte2bVtJ0j//+U9J0kcffaRz585p5cqVPu8mS96pdbkUFRUpOzvb5wf0q6++kqQy7yCsU6eOqlWrpsLCwkt6F1WaESNG6Nlnn1WLFi3KDNexY8f0xz/+UVOnTtWvfvUrZ7r36LC4oqIi3XvvvTp+/LjWrl2rG264wef73huHqlevfknrEBcXJ0nKyckp9fveo4zi4Stvu0oX35mWvIFAuvhOuvg75tjYWGVlZalHjx7lvsMbPHiwz+mxsowePdr5ix3Lli1TeHi40tPTFRYW5syTlpbm85iYmBgVFRUpJyfH511+8btTpSuzz0jf/0jF+4ciSo63rGml8b4uVapUKXfdYmNjderUqcu6DRYtWqSUlJSA5vUGtvh6ey9HSBc/FrF//36fNy0xMTH68ssvVVRU5BOI3bt3+zxXoPvM9+Fd1p49e5zLDNLFm2lycnJ02223OdOWLl2qpk2bavny5T77SWpqaoWXO3DgQPXq1UvJycmqVq2a3x2mmzZt8tmONjk5Oc7vgMaNG6tx48bKzMxUdna2c2q9a9euevjhh7VkyRIVFhaqa9euzuOvRCNiY2P1yCOP6JFHHtGePXvUpk0bvfjii3r33XedeQ4ePKiCggLnTEKgKhTKjIwMde/e3e8H+5NPPpEk/cd//Iekf7+DLP6OMT8//7LubCXNmTNHs2fPdpY7Z84cValSRT169Ch1/uDgYA0ZMkQLFy4s9S7ew4cPl3lqqSwPPPCAgoODnWupZS3XO8biSvvA+dSpU5Wenq7Vq1eXekolISFBsbGxmjlzpkaOHOl39FjeOkRHR6tRo0Zl/tWgb7/9VitWrHBu8z5x4oQWLFigNm3aWN/Vx8bGKjMzUwUFBc71lVWrVun//u//fEKZlJSkTz75RG+88YbGjRvn8xxnzpxRUVGRIiIiJF28RhnIkW/Dhg2d/w4ODpbL5VJhYaEzbf/+/T5300oXr489+eSTmjt3rnPnqSS/vzZ1JfYZSYqIiKjQNZmSGjZsqNatW2vBggV6/PHHnf1g/fr12rlzZ0B/calu3brq3r27Xn/9dU2ePNnvNGjxdUtKStLTTz+t9PR0v6PP48ePKzIy0ueaYSC81ygrom3btqpVq5beeOMNpaSkOMt87733/PaV//zP/9Rnn32mRYsWOdcpL1y4oN/+9reKjIxUt27dJAW+z3wfbdu2VZ06dTRv3jyfa9jvvPOO3xvM4r8vvL93P//8c23evNnvlHYgRo0apRMnTmjy5MmqXr26pk+f7nzPe40yECV//rt06SKPx6Pc3Fzn8lCbNm1UrVo1TZs2zfmYTWnr5XWpjTh9+rSCgoJ8/rhLbGysqlWr5vcxs23btkmSOnXqVKFlVGhvnjx5sk6fPq1BgwYpLi5OBQUF2rRpkxYtWqQmTZo47wh79eql0NBQ9evXT+PHj9epU6f0xhtvqG7dus5R5+UUHh6uTz/9VKNHj1b79u21evVqffzxx3riiSesv7imTZumjIwMtW/fXmPHjlXLli2Vl5enL774QmvXrg34NIRXTExMuX9Ltnr16uratatmzJih8+fPKzo6Wp999pnfUd3OnTv1zDPPqGvXrsrNzfV5VyRdPP0XFBSkN998Uz/96U/VqlUrpaSkKDo6WgcPHlRGRoaqV6+ujz76yDqeAQMGaMWKFaWet7/llls0ZswYbdmyRfXq1dPbb7+tQ4cOlbszP/DAA1q6dKl69+6tpKQk7du3T++++65zBOz1s5/9TIsXL9aECROUkZGhzp07q7CwULt379bixYuVnp7unK24lGuUffr00UsvvaTevXtr5MiRys3N1auvvqpmzZr5fO4rISFBQ4YM0axZs3T06FHn4yHeo+fi2+Vy7zPe5S9atEgPP/yw2rVrp8jISPXr10/Sxc9Zrl+/vtxTtc8//7wGDBigzp07KyUlRceOHdOcOXPUunVrnTp1KqBxvPrqq7rrrrsUHx+vsWPHqmnTpjp06JA2b96sb775RllZWZKkRx99VCtXrlTfvn2VnJyshIQEfffdd9q5c6eWLl2q/fv3V/jU1qVcowwNDdXTTz+tyZMny+12KykpSfv379c777yj2NhYn9dt3Lhxev3115WcnKxt27apSZMmWrp0qTZu3KhZs2apWrVqkgLfZ8riPcoq694I6eJR+7PPPqvx48fL7XZr+PDhysnJUVpamt81yr59+2r58uUaNGiQ+vTpo5ycHM2bN08tW7YM+HUtadKkSTpx4oSefPJJRUVF6YknnpB06dcopYuhfO+99+RyuZxTscHBwerUqZPS09PVvXt3nxuXLmcjvvrqK/Xo0UNJSUlq2bKlQkJCtGLFCh06dEj33HOPz7xr1qxR48aNK/bREKliHw9ZvXq1uf/++01cXJyJjIw0oaGhplmzZmby5Mnm0KFDPvOuXLnS3HrrrSY8PNw0adLETJ8+3bz99tt+txZ7P2JRkiQzceJEn2ne24KL3wY8evRoExERYfbt22d69eplbrjhBlOvXj2TmppqCgsL/Z6z+K3+xhhz6NAhM3HiRNOoUSNTpUoVU79+fdOjRw8zf/78crdHWWMvrrSPh3zzzTdm0KBB5sYbbzRRUVFm2LBh5ttvv/UZX3m3ahe3fft2M3jwYFOrVi0TFhZmYmJiTFJSkvnjH/9Y7jp88cUXRpLfRzS865aenm5uvfVWExYWZuLi4sySJUt85ivt4yHGGPPiiy+a6OhoExYWZjp37my2bt3qd5u3MRdvlZ8+fbpp1aqVCQsLMzVq1DAJCQlm6tSpJj8/v9zxl+ett94yzZs3d8aflpZmUlNT/bbhd999ZyZOnGhq1qxpIiMjzcCBA80//vEPI8lMmzbNZ95A9pnit+QXV/LWdmOMOXXqlBk5cqS58cYb/T7OkZCQYOrXrx/Qun7wwQcmLi7OhIWFmdatW5uVK1eaIUOGmLi4OL/ll3Ur/b59+8yoUaNM/fr1TZUqVUx0dLTp27evWbp0qc98J0+eNI8//rhp1qyZCQ0NNbVr1zadOnUyM2fOdD4uZltWaT+Ll2r27NkmJibGhIWFmTvvvNNs3LjRJCQkmN69e/vMd+jQIZOSkmJq165tQkNDTXx8vM/r4BXoPlPax0Nq165tOnToENC4586da26++WYTFhZm2rZtazZs2OD3M1JUVGSef/55Z/1uv/12s2rVKjN69OhSP/ZT2rYua1987LHHjCQzZ86cgMZrs2vXLufjUMU9++yzRpJ56qmn/B4TaCPK+3jIkSNHzMSJE01cXJyJiIgwUVFRpn379mbx4sU+yyssLDQNGjQw//u//1vh9atQKK9G3lDi0rndbnPfffdV9jCuOtu3b/f77N0P6cSJEyYkJOR7/SK77bbbzN13330ZR3X1KywsNDVr1jQPPPDAD7pcbyxWrVr1gy4XgVmxYoWpWrWq+fbbbyv82B/V/2YLpXv++ee1aNEin1vkf2zOnDnjN23WrFkKCgryuQnhh7RhwwZFR0dr7Nix5c57/vx5XbhwwWfaunXrlJWVddn/t01Xk7Nnz/qdll6wYIHy8vJ+8PXOyMhQx44dL9tf78HlNX36dE2aNKnCp/glyWVK7mXXmOTkZC1duvSSz9cD0sUbp7Zt26af/OQnCgkJ0erVq7V69Wrn2tbVbv/+/br77rt13333qWHDhtq9e7fmzZunqKgo/fWvf3U+CnS9WbdunR566CENGzZMtWrV0hdffKG33npLLVq00LZt2/hD77gsLsvHQ4BrXadOnbRmzRo988wzOnXqlBo3bqynn35aTz75ZGUPLSA1atRQQkKC3nzzTR0+fFgRERHq06ePpk2bdt1GUrp480yjRo00e/Zs5eXlqWbNmho1apSmTZtGJHHZXPNHlAAAXElcowQAwIJQAgBgcc1dowzkr4wAAK5e19od9hxRAgBgQSgBALAglAAAWBBKAAAsCCUAABaEEgAAC0IJAIAFoQQAwIJQAgBgQSgBALAglAAAWBBKAAAsCCUAABaEEgAAC0IJAIAFoQQAwIJQAgBgQSgBALAglAAAWBBKAAAsCCUAABaEEgAAC0IJAIAFoQQAwCKksgdwNfJ4PFf0+d1u9xV9funKrwOuHuxPgXGvir3yC3kp5oov4nr4/XSt4YgSAAALQgkAgAWhBADAglACAGBBKAEAsCCUAABYEEoAACwIJQAAFoQSAAALQgkAgAWhBADAglACAGBBKAEAsCCUAABYEEoAACwIJQAAFoQSAAALQgkAgAWhBADAglACAGBBKAEAsCCUAABYEEoAACxCKnsAuHa53e4r+vwej+eKPr905dcBgeO1wNWKI0oAACwIJQAAFoQSAAALQgkAgAWhBADAglACAGBBKAEAsCCUAABYEEoAACwIJQAAFoQSAAALQgkAgAWhBADAglACAGBBKAEAsCCUAABYEEoAACwIJQAAFoQSAAALQgkAgAWhBADAglACAGBBKAEAsCCUAABYhFT2AIDK5PF4KnsI+IHwWuNScUQJAIAFoQQAwIJQAgBgQSgBALAglAAAWBBKAAAsCCUAABaEEgAAC0IJAIAFoQQAwIJQAgBgQSgBALAglAAAWBBKAAAsCCUAABaEEgAAC0IJAIAFoQQAwIJQAgBgQSgBALAglAAAWBBKAAAsCCUAABaEEgAAi5DKHgBQFrfbXdlDACqEffb6xBElAAAWhBIAAAtCCQCABaEEAMCCUAIAYEEoAQCwIJQAAFgQSgAALAglAAAWhBIAAAtCCQCABaEEAMCCUAIAYEEoAQCwIJQAAFgQSgAALAglAAAWhBIAAAtCCQCABaEEAMCCUAIAYEEoAQCwIJQAAFiEVPYAfow8Hk9lD+GawHbCteZ62GfdbndlD+GqwxElAAAWhBIAAAtCCQCABaEEAMCCUAIAYEEoAQCwIJQAAFgQSgAALAglAAAWhBIAAAtCCQCABaEEAMCCUAIAYEEoAQCwIJQAAFgQSgAALAglAAAWhBIAAAtCCQCABaEEAMCCUAIAYEEoAQCwIJQAAFgQSgAALEIqewA/Rm63u7KHcE1gO+Fy8ng8V3wZP8Q++0OsB3xxRAkAgAWhBADAglACAGBBKAEAsCCUAABYEEoAACwIJQAAFoQSAAALQgkAgAWhBADAglACAGBBKAEAsCCUAABYEEoAACwIJQAAFoQSAAALQgkAgAWhBADAglACAGBBKAEAsCCUAABYEEoAACwIJQAAFoQSAACLkMoewI+Rx+Op7CEAVx23213ZQ7gmJH2dXNlD+NHhiBIAAAtCCQCABaEEAMCCUAIAYEEoAQCwIJQAAFgQSgAALAglAAAWhBIAAAtCCQCABaEEAMCCUAIAYEEoAQCwIJQAAFgQSgAALAglAAAWhBIAAAtCCQCABaEEAMCCUAIAYEEoAQCwIJQAAFgQSgAALEIqewBXI7fbXdlDAK4qHo+nsoeA/29x43eu6PO7xe+/kjiiBADAglACAGBBKAEAsCCUAABYEEoAACwIJQAAFoQSAAALQgkAgAWhBADAglACAGBBKAEAsCCUAABYEEoAACwIJQAAFoQSAAALQgkAgAWhBADAglACAGBBKAEAsCCUAABYEEoAACwIJQAAFoQSAAALQgkAgEVIZQ/gauTxeCp7CEDA3G53ZQ8BuK5xRAkAgAWhBADAglACAGBBKAEAsCCUAABYEEoAACwIJQAAFoQSAAALQgkAgAWhBADAglACAGBBKAEAsCCUAABYEEoAACwIJQAAFoQSAAALQgkAgAWhBADAglACAGBBKAEAsCCUAABYEEoAACwIJQAAFoQSAACLkMoewI+R2+2+4svweDxXfBk/xHoAlwv7Ky4VR5QAAFgQSgAALAglAAAWhBIAAAtCCQCABaEEAMCCUAIAYEEoAQCwIJQAAFgQSgAALAglAAAWhBIAAAtCCQCABaEEAMCCUAIAYEEoAQCwIJQAAFgQSgAALAglAAAWhBIAAAtCCQCABaEEAMCCUAIAYBFS2QMAyuLxeCp7CNcEt9td2UO4LK6H1/t6eS3giyNKAAAsCCUAABaEEgAAC0IJAIAFoQQAwIJQAgBgQSgBALAglAAAWBBKAAAsCCUAABaEEgAAC0IJAIAFoQQAwIJQAgBgQSgBALAglAAAWBBKAAAsCCUAABaEEgAAC0IJAIAFoQQAwIJQAgBgQSgBALAglAAAWIRU9gCAyuR2u6/4MjwezxVfxpX2Q2ynK+16eB1QOTiiBADAglACAGBBKAEAsCCUAABYEEoAACwIJQAAFoQSAAALQgkAgAWhBADAglACAGBBKAEAsCCUAABYEEoAACwIJQAAFoQSAAALQgkAgAWhBADAglACAGBBKAEAsCCUAABYEEoAACwIJQAAFoQSAAALQgkAgEVIZQ8A1y6Px1PZQ4B4Ha4mvBbXJ44oAQCwIJQAAFgQSgAALAglAAAWhBIAAAtCCQCABaEEAMCCUAIAYEEoAQCwIJQAAFgQSgAALAglAAAWhBIAAAtCCQCABaEEAMCCUAIAYEEoAQCwIJQAAFgQSgAALAglAAAWhBIAAAtCCQCABaEEAMDCZYwxlT2IioiJiansIQAAvocDBw5U9hAqhCNKAAAsCCUAABaEEgAAC0IJAIAFoQQAwIJQAgBgQSgBALAglAAAWBBKAAAsCCUAABaEEgAAC0IJAIAFoQQAwIJQAgBgQSgBALAglAAAWBBKAAAsCCUAABaEEgAAC0IJAIAFoQQAwIJQAgBgQSgBALAglAAAWLiMMaayBwEAwNWKI0oAACwIJQAAFoQSAAALQgkAgAWhBADAglACAGBBKAEAsCCUAABYEEoAACz+H9c4papzz1W8AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "env = MazeEnv(size=15, view=7, max_steps=200, seed=42)\n",
    "obs, info = env.reset()\n",
    "plt.imshow(env.render())\n",
    "plt.title(\"Sample Maze (blue=agent, green=goal, dark=walls)\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88bcf6d0",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Vectorized environments for faster training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "94390e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_env(seed_offset=0, size=25):  # Increased from 15 to 25\n",
    "    def thunk():\n",
    "        # Scale max_steps with maze size for larger mazes\n",
    "        max_steps = size * size // 2  # More steps for larger mazes\n",
    "        env = MazeEnv(size=size, view=7, max_steps=max_steps, seed=int(time.time())+seed_offset)\n",
    "        return Monitor(env)\n",
    "    return thunk\n",
    "\n",
    "# Progressive maze sizes - start smaller and work up\n",
    "MAZE_SIZE = 21  # Start with 21, then scale to 25, 31, 41\n",
    "NUM_ENVS = 8   # More environments for complex mazes\n",
    "vec_env = DummyVecEnv([make_env(i, size=MAZE_SIZE) for i in range(NUM_ENVS)])\n",
    "test_env = DummyVecEnv([make_env(10_000, size=MAZE_SIZE)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1d91f5",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Train PPO (CNN policy)\n",
    "Logs go to `./tb_logs/ppo_maze` ‚Äì you can launch TensorBoard to watch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2c8932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöÄ FIXED PPO Training with Curriculum Learning\n",
    "# This replaces the problematic training that was getting 0% success\n",
    "\n",
    "print(\"üîß Setting up FIXED training with curriculum learning...\")\n",
    "\n",
    "# Clean up any existing environments\n",
    "if 'vec_env' in locals():\n",
    "    try:\n",
    "        vec_env.close()\n",
    "    except:\n",
    "        pass\n",
    "if 'test_env' in locals():\n",
    "    try:\n",
    "        test_env.close()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# 1. IMPROVED MAZE ENVIRONMENT with better rewards\n",
    "class FixedMazeEnv(gym.Env):\n",
    "    def __init__(self, size=9, view=7, max_steps=None, seed=None):\n",
    "        super().__init__()\n",
    "        self.size = size\n",
    "        self.view = view\n",
    "        self.max_steps = max_steps or (size * 8)  # Reasonable step limit\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "        \n",
    "        self.action_space = spaces.Discrete(4)\n",
    "        self.observation_space = spaces.Box(low=0.0, high=1.0, shape=(4, view, view), dtype=np.float32)\n",
    "        \n",
    "        self.grid = None\n",
    "        self.agent = None\n",
    "        self.goal = None\n",
    "        self.steps = 0\n",
    "        self.prev_dist = None\n",
    "\n",
    "    def _reset_level(self):\n",
    "        self.grid = generate_maze(self.size, self.size, self.rng)\n",
    "        free = np.argwhere(self.grid == 0)\n",
    "        \n",
    "        # Find good start/goal positions\n",
    "        for _ in range(100):\n",
    "            a_idx = self.rng.integers(len(free))\n",
    "            g_idx = self.rng.integers(len(free))\n",
    "            self.agent = tuple(free[a_idx])\n",
    "            self.goal = tuple(free[g_idx])\n",
    "            \n",
    "            L = astar_length(self.grid, self.agent, self.goal)\n",
    "            if L is not None and L >= max(self.size//3, 3):\n",
    "                break\n",
    "\n",
    "    def _obs(self):\n",
    "        pad = self.view//2\n",
    "        H, W = self.grid.shape\n",
    "        padded = np.ones((H+2*pad, W+2*pad), dtype=np.int8)\n",
    "        padded[pad:pad+H, pad:pad+W] = self.grid\n",
    "\n",
    "        ar, ac = self.agent\n",
    "        gr, gc = self.goal\n",
    "        crop = padded[ar:ar+2*pad+1, ac:ac+2*pad+1]\n",
    "\n",
    "        empty = (crop == 0).astype(np.float32)\n",
    "        wall = (crop == 1).astype(np.float32)\n",
    "\n",
    "        goal_layer = np.zeros_like(empty, dtype=np.float32)\n",
    "        ga, gb = pad+(gr-ar), pad+(gc-ac)\n",
    "        if 0 <= ga < self.view and 0 <= gb < self.view:\n",
    "            goal_layer[ga, gb] = 1.0\n",
    "\n",
    "        agent_layer = np.zeros_like(empty, dtype=np.float32)\n",
    "        agent_layer[pad, pad] = 1.0\n",
    "\n",
    "        return np.stack([empty, wall, goal_layer, agent_layer], axis=0)\n",
    "\n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        if seed is not None:\n",
    "            self.rng = np.random.default_rng(seed)\n",
    "        self._reset_level()\n",
    "        self.steps = 0\n",
    "        self.prev_dist = abs(self.agent[0] - self.goal[0]) + abs(self.agent[1] - self.goal[1])\n",
    "        return self._obs(), {}\n",
    "\n",
    "    def step(self, action):\n",
    "        self.steps += 1\n",
    "        r, c = self.agent\n",
    "        drdc = [(-1,0),(1,0),(0,-1),(0,1)][int(action)]\n",
    "        nr, nc = r+drdc[0], c+drdc[1]\n",
    "\n",
    "        reward = -0.01  # Small step penalty\n",
    "        done = False\n",
    "\n",
    "        if 0 <= nr < self.grid.shape[0] and 0 <= nc < self.grid.shape[1] and self.grid[nr, nc] == 0:\n",
    "            # Valid move\n",
    "            old_dist = self.prev_dist\n",
    "            new_dist = abs(nr-self.goal[0]) + abs(nc-self.goal[1])\n",
    "            \n",
    "            # FIXED: Better reward shaping\n",
    "            if new_dist < old_dist:\n",
    "                reward += 0.1  # Reward for getting closer\n",
    "            elif new_dist > old_dist:\n",
    "                reward -= 0.02  # Small penalty for moving away\n",
    "            \n",
    "            # Proximity bonus\n",
    "            if new_dist <= 2:\n",
    "                reward += 0.2\n",
    "            elif new_dist <= 5:\n",
    "                reward += 0.1\n",
    "            \n",
    "            self.agent = (nr, nc)\n",
    "            self.prev_dist = new_dist\n",
    "        else:\n",
    "            # Wall bump\n",
    "            reward -= 0.1\n",
    "\n",
    "        if self.agent == self.goal:\n",
    "            reward += 10.0  # FIXED: Reasonable goal reward\n",
    "            done = True\n",
    "\n",
    "        truncated = self.steps >= self.max_steps\n",
    "        return self._obs(), reward, done, truncated, {}\n",
    "\n",
    "    def render(self):\n",
    "        H, W = self.grid.shape\n",
    "        img = np.zeros((H, W, 3), dtype=np.uint8)\n",
    "        img[self.grid == 1] = (30, 30, 30)\n",
    "        img[self.grid == 0] = (220, 220, 220)\n",
    "        gr, gc = self.goal\n",
    "        img[gr, gc] = (50, 190, 60)\n",
    "        ar, ac = self.agent\n",
    "        img[ar, ac] = (30, 144, 255)\n",
    "        return img\n",
    "\n",
    "# 2. CURRICULUM LEARNING CALLBACK with FIXED success detection\n",
    "class FixedCurriculumCallback(BaseCallback):\n",
    "    def __init__(self, check_freq: int = 20000):\n",
    "        super().__init__()\n",
    "        self.check_freq = check_freq\n",
    "        self.current_size = 9  # Start small!\n",
    "        self.success_threshold = 0.7\n",
    "        \n",
    "    def _on_step(self) -> bool:\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            if hasattr(self.model, 'ep_info_buffer') and self.model.ep_info_buffer:\n",
    "                recent = list(self.model.ep_info_buffer)[-50:]\n",
    "                \n",
    "                # FIXED: Correct success detection (goal reward = 10.0)\n",
    "                successes = sum(1 for ep in recent if ep.get('r', 0) > 5.0)\n",
    "                success_rate = successes / len(recent) if recent else 0\n",
    "                mean_reward = np.mean([ep['r'] for ep in recent]) if recent else 0\n",
    "                mean_length = np.mean([ep['l'] for ep in recent]) if recent else 0\n",
    "                \n",
    "                print(f\"\\\\nStep {self.num_timesteps:,} | Size: {self.current_size}√ó{self.current_size}\")\n",
    "                print(f\"Success: {success_rate:.3f} | Reward: {mean_reward:.2f} | Length: {mean_length:.1f}\")\n",
    "                \n",
    "                if torch.cuda.is_available():\n",
    "                    memory_allocated = torch.cuda.memory_allocated() // 1024**2\n",
    "                    print(f\"GPU Memory: {memory_allocated} MB allocated\")\n",
    "                \n",
    "                # Curriculum progression\n",
    "                if success_rate >= self.success_threshold and self.current_size < 21:\n",
    "                    old_size = self.current_size\n",
    "                    self.current_size = min(self.current_size + 2, 21)\n",
    "                    print(f\"üéØ ADVANCING! {old_size}√ó{old_size} ‚Üí {self.current_size}√ó{self.current_size}\")\n",
    "                    \n",
    "                    # Update environments\n",
    "                    self._update_envs()\n",
    "                    \n",
    "        return True\n",
    "    \n",
    "    def _update_envs(self):\n",
    "        # Update training environment sizes\n",
    "        if hasattr(self.training_env, 'envs'):\n",
    "            for env_wrapper in self.training_env.envs:\n",
    "                if hasattr(env_wrapper, 'env') and hasattr(env_wrapper.env, 'size'):\n",
    "                    env_wrapper.env.size = self.current_size\n",
    "                    env_wrapper.env.max_steps = self.current_size * 8\n",
    "\n",
    "def make_fixed_env(seed_offset=0, initial_size=9):\n",
    "    def thunk():\n",
    "        env = FixedMazeEnv(size=initial_size, view=7, seed=int(time.time())+seed_offset)\n",
    "        return Monitor(env)\n",
    "    return thunk\n",
    "\n",
    "# 3. CREATE TRAINING SETUP\n",
    "print(\"Creating curriculum learning environments...\")\n",
    "NUM_ENVS = 8\n",
    "INITIAL_SIZE = 9\n",
    "\n",
    "vec_env = DummyVecEnv([make_fixed_env(i, INITIAL_SIZE) for i in range(NUM_ENVS)])\n",
    "test_env = DummyVecEnv([make_fixed_env(10_000, INITIAL_SIZE)])\n",
    "\n",
    "# 4. CREATE MODEL with FIXED hyperparameters\n",
    "print(\"Creating PPO model with optimized settings...\")\n",
    "\n",
    "model = PPO(\n",
    "    policy=\"CnnPolicy\",\n",
    "    env=vec_env,\n",
    "    n_steps=1024,\n",
    "    batch_size=256,\n",
    "    learning_rate=3e-4,\n",
    "    gamma=0.99,\n",
    "    gae_lambda=0.95,\n",
    "    clip_range=0.2,\n",
    "    vf_coef=0.5,\n",
    "    ent_coef=0.01,\n",
    "    policy_kwargs=dict(\n",
    "        features_extractor_class=MazeCNN,\n",
    "        features_extractor_kwargs=dict(features_dim=256),\n",
    "        normalize_images=False,\n",
    "    ),\n",
    "    verbose=0,\n",
    "    tensorboard_log=\"tb_logs/fixed_maze\",\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Fixed training setup complete!\")\n",
    "print(\"üìä Key improvements:\")\n",
    "print(\"- Start with 9√ó9 mazes (much easier)\")  \n",
    "print(\"- Goal reward: +10 (reasonable)\")\n",
    "print(\"- Progress reward: +0.1 for getting closer\")\n",
    "print(\"- Fixed success detection: reward > 5.0\")\n",
    "print(\"- Curriculum: advance at 70% success rate\")\n",
    "\n",
    "# 5. START TRAINING\n",
    "print(\"\\\\nüöÄ Starting curriculum training...\")\n",
    "callback = FixedCurriculumCallback(check_freq=20000)\n",
    "\n",
    "total_timesteps = 1_000_000\n",
    "print(f\"Training for {total_timesteps:,} timesteps...\")\n",
    "\n",
    "# This should show immediate improvement!\n",
    "model.learn(total_timesteps=total_timesteps, callback=callback)\n",
    "\n",
    "model.save(\"ppo_fixed_maze_model\")\n",
    "print(\"\\\\n‚úÖ Training completed! You should see much better results now!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3819e0f2",
   "metadata": {},
   "source": [
    "\n",
    "## 6. Evaluate on held-out mazes\n",
    "We measure:\n",
    "- Success rate\n",
    "- Average steps\n",
    "- Ratio of steps to A* shortest path (lower is better; target ‚â§ 1.5√ó)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "52ce95dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Evaluating on 21√ó21 mazes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval 21√ó21: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:11<00:00,  2.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Large Maze Results (21√ó21):\n",
      "Success Rate: 0.000\n",
      "Average Steps: 220.0\n",
      "Steps/A* Ratio: N/A\n",
      "\n",
      "üìä Progressive Size Evaluation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval 15√ó15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:02<00:00,  4.74it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported format string passed to NoneType.__format__",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[103]\u001b[39m\u001b[32m, line 51\u001b[39m\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m test_size <= MAZE_SIZE:\n\u001b[32m     50\u001b[39m     sr, avg_steps, avg_ratio = evaluate_large_mazes(model, episodes=\u001b[32m10\u001b[39m, size=test_size)\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m√ó\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: Success=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msr\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Steps=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_steps\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Ratio=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_ratio\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f if avg_ratio else \u001b[39m\u001b[33m'\u001b[39m\u001b[33mN/A\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: unsupported format string passed to NoneType.__format__"
     ]
    }
   ],
   "source": [
    "# Enhanced Evaluation for Large Mazes\n",
    "print(f\"üîç Evaluating on {MAZE_SIZE}√ó{MAZE_SIZE} mazes...\")\n",
    "\n",
    "def evaluate_large_mazes(model, episodes=30, size=25):\n",
    "    \"\"\"Evaluate model on large mazes with proper scaling\"\"\"\n",
    "    successes, total_steps, ratios = 0, 0, []\n",
    "    max_steps = size * size // 2  # Scale with maze size\n",
    "    \n",
    "    for _ in trange(episodes, desc=f\"Eval {size}√ó{size}\"):\n",
    "        env = MazeEnv(size=size, view=7, max_steps=max_steps, seed=np.random.randint(1e9))\n",
    "        obs, _ = env.reset()\n",
    "        start = env.agent\n",
    "        goal = env.goal\n",
    "        \n",
    "        # Calculate A* optimal path length\n",
    "        L_astar = astar_length(env.grid, start, goal)\n",
    "        \n",
    "        # Run episode with the model\n",
    "        steps = 0\n",
    "        with torch.no_grad():\n",
    "            while True:\n",
    "                action, _ = model.predict(obs, deterministic=True)\n",
    "                obs, reward, terminated, truncated, info = env.step(action)\n",
    "                steps += 1\n",
    "                if terminated or truncated:\n",
    "                    break\n",
    "        \n",
    "        total_steps += steps\n",
    "        if env.agent == env.goal:  # Success\n",
    "            successes += 1\n",
    "            if L_astar is not None and L_astar > 0:\n",
    "                ratios.append(steps / L_astar)\n",
    "    \n",
    "    sr = successes / episodes\n",
    "    avg_steps = total_steps / episodes\n",
    "    avg_ratio = float(np.mean(ratios)) if ratios else None\n",
    "    return sr, avg_steps, avg_ratio\n",
    "\n",
    "# Evaluate the trained model\n",
    "sr, avg_steps, avg_ratio = evaluate_large_mazes(model, episodes=30, size=MAZE_SIZE)\n",
    "print(f\"üéØ Large Maze Results ({MAZE_SIZE}√ó{MAZE_SIZE}):\")\n",
    "print(f\"Success Rate: {sr:.3f}\")\n",
    "print(f\"Average Steps: {avg_steps:.1f}\")\n",
    "print(f\"Steps/A* Ratio: {avg_ratio:.2f}\" if avg_ratio else \"Steps/A* Ratio: N/A\")\n",
    "\n",
    "# Progressive evaluation - test different sizes\n",
    "print(\"\\nüìä Progressive Size Evaluation:\")\n",
    "for test_size in [15, 21, 25]:\n",
    "    if test_size <= MAZE_SIZE:\n",
    "        sr, avg_steps, avg_ratio = evaluate_large_mazes(model, episodes=10, size=test_size)\n",
    "        print(f\"{test_size}√ó{test_size}: Success={sr:.2f}, Steps={avg_steps:.1f}, Ratio={avg_ratio:.2f if avg_ratio else 'N/A'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "bd2175af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating trained model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval 15x15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [00:16<00:00,  2.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: Success Rate: 0.060, Avg Steps: 188.6, Steps/A* Ratio: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def evaluate_with_astar(model, episodes=200, size=15):\n",
    "    \"\"\"Evaluate the model and compare performance to A* shortest path.\"\"\"\n",
    "    successes, total_steps, ratios = 0, 0, []\n",
    "    \n",
    "    for _ in trange(episodes, desc=f\"Eval {size}x{size}\"):\n",
    "        env = MazeEnv(size=size, view=7, max_steps=200, seed=np.random.randint(1e9))\n",
    "        obs, _ = env.reset()\n",
    "        start = env.agent\n",
    "        goal = env.goal\n",
    "        \n",
    "        # Calculate A* optimal path length\n",
    "        L_astar = astar_length(env.grid, start, goal)\n",
    "        \n",
    "        # Run episode with the model\n",
    "        steps = 0\n",
    "        with torch.no_grad():\n",
    "            while True:\n",
    "                action, _ = model.predict(obs, deterministic=True)\n",
    "                obs, reward, terminated, truncated, info = env.step(action)\n",
    "                steps += 1\n",
    "                if terminated or truncated:\n",
    "                    break\n",
    "        \n",
    "        total_steps += steps\n",
    "        if env.agent == env.goal:  # Success\n",
    "            successes += 1\n",
    "            if L_astar is not None and L_astar > 0:\n",
    "                ratios.append(steps / L_astar)\n",
    "    \n",
    "    sr = successes / episodes\n",
    "    avg_steps = total_steps / episodes\n",
    "    avg_ratio = float(np.mean(ratios)) if ratios else None\n",
    "    return sr, avg_steps, avg_ratio\n",
    "\n",
    "# Run evaluation after training\n",
    "print(\"Evaluating trained model...\")\n",
    "sr, avg_steps, avg_ratio = evaluate_with_astar(model, episodes=50, size=15)\n",
    "print(f\"Results: Success Rate: {sr:.3f}, Avg Steps: {avg_steps:.1f}, Steps/A* Ratio: {avg_ratio:.2f}\" if avg_ratio else f\"Results: Success Rate: {sr:.3f}, Avg Steps: {avg_steps:.1f}, Steps/A* Ratio: N/A\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3621159c",
   "metadata": {},
   "source": [
    "\n",
    "## 7. Record a few rollouts to GIF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "286a25aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating demo video...\n",
      "‚úÖ Saved videos/ppo_maze_demo.gif\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def render_episode(model, size=15, max_steps=1800):\n",
    "    \"\"\"Render a complete episode and return frames for GIF creation.\"\"\"\n",
    "    env = MazeEnv(size=size, view=7, max_steps=max_steps, seed=np.random.randint(1e9))\n",
    "    frames = []\n",
    "    obs, _ = env.reset()\n",
    "    frames.append(env.render())\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_steps):\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            obs, reward, terminated, truncated, info = env.step(action)\n",
    "            frames.append(env.render())\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "    \n",
    "    return frames\n",
    "\n",
    "# Create video directory and generate demo\n",
    "os.makedirs(\"videos\", exist_ok=True)\n",
    "print(\"Generating demo video...\")\n",
    "frames = render_episode(model, size=9)\n",
    "imageio.mimsave(\"videos/ppo_maze_demo.gif\", frames, duration=2)\n",
    "print(\"‚úÖ Saved videos/ppo_maze_demo.gif\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa871c01",
   "metadata": {},
   "source": [
    "\n",
    "## 8. (Optional) Try an LSTM policy (helps with partial observability)\n",
    "Re-train with `MlpLstmPolicy` or `CnnLstmPolicy` for better memory at junctions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc9e14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example (commented out by default):\n",
    "# model_lstm = PPO(\n",
    "#     policy=\"CnnLstmPolicy\",\n",
    "#     env=vec_env,\n",
    "#     n_steps=256,\n",
    "#     batch_size=4096,\n",
    "#     learning_rate=3e-4,\n",
    "#     gamma=0.995,\n",
    "#     gae_lambda=0.95,\n",
    "#     clip_range=0.2,\n",
    "#     vf_coef=0.5,\n",
    "#     ent_coef=0.01,\n",
    "#     verbose=1,\n",
    "#     tensorboard_log=\"tb_logs/ppo_maze_lstm\",\n",
    "#     device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "# )\n",
    "# model_lstm.learn(total_timesteps=1_000_000)\n",
    "# model_lstm.save(\"ppo_maze_partial_lstm\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db4d916",
   "metadata": {},
   "source": [
    "\n",
    "## 9. TensorBoard\n",
    "In a separate terminal:\n",
    "```bash\n",
    "tensorboard --logdir tb_logs\n",
    "```\n",
    "Then open the URL it prints (usually http://localhost:6006).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
